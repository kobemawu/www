{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similarity.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kobemawu/www/blob/master/Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aVVn6hB9Zei"
      },
      "source": [
        "# Preprocessing and calculate similarity\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆã®ç›®æ¨™ã¯è‡ªåŠ›ã§æ–‡æ›¸ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨  \n",
        "æœ€çµ‚çš„ã«Wikipediaã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å›½ã®é¡ä¼¼åº¦ã‚’æ¸¬ã‚Š  \n",
        "æ—¥æœ¬ã¨ä¼¼ã¦ã„ã‚‹å›½ã‚’æ¢ã™"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5-315WVmN64",
        "outputId": "7fcd0c1f-389a-4791-a9f2-3254876bd499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8iK7xBDTmtf"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBUvXHj7mRls",
        "outputId": "715b4836-8e05-4313-d0e5-cf8e81547343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpg2beFhKA2R"
      },
      "source": [
        "## 1. Calculate similarity\n",
        "\n",
        "ä»¥ä¸‹ã®ä¸‰ã¤ã®æ–‡ã‚’è€ƒãˆã‚‹  \n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "\n",
        "Doc Aã¨Doc Bã¯ä¼¼ã¦ã„ãã†ã ãŒã€Doc Cã¯Doc Aã¨ã‚‚Doc Bã¨ã‚‚ä¼¼ã¦ã„ãªã•ãã†  \n",
        "ã“ã‚Œã‚’é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ç¢ºã‹ã‚ã‚‹\n",
        "\n",
        "é¡ä¼¼åº¦ã®è¨ˆç®—ã®ä»•æ–¹ã¯ã„ãã¤ã‹ã‚ã‚‹\n",
        "\n",
        "- é›†åˆãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦\n",
        "  - Jaccardä¿‚æ•°\n",
        "  - Diceä¿‚æ•°\n",
        "  - Simpsonä¿‚æ•°\n",
        "- ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦\n",
        "  - ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢\n",
        "  - ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rpPCaUeOWE"
      },
      "source": [
        "### é›†åˆãƒ™ãƒ¼ã‚¹\n",
        "\n",
        "æ–‡æ›¸ã‚’å˜èªã®é›†åˆã«å¤‰æ›ã™ã‚‹  \n",
        "é›†åˆãªã®ã§é‡è¤‡ã—ãŸå˜èªã¯å‰Šé™¤ã™ã‚‹  \n",
        "å‰å‡¦ç†ã¯ä»Šå›ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹   \n",
        "\n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "â†“    \n",
        "Set A : {'a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'}  \n",
        "Set B : {'an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'}  \n",
        "Set C : {'basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'}  \n",
        "\n",
        "ã“ã®é›†åˆãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’è¡¨ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uts5Ns2eKDaW"
      },
      "source": [
        "#### Jaccardä¿‚æ•°\n",
        "Jaccardä¿‚æ•°ã¯äºŒã¤ã®é›†åˆA,Bã«å¯¾ã—ã¦å®šç¾©ã•ã‚Œã‚‹é¡ä¼¼åº¦ã§ã‚ã‚‹  \n",
        "è¨ˆç®—å¼ã¯ä»¥ä¸‹ã®é€šã‚Š\n",
        "\n",
        "\\begin{equation}\n",
        "J(A,B)=\\dfrac{|A\\cap B|}{|A \\cup B|}\n",
        "\\end{equation}\n",
        "\n",
        "å…±é€šéƒ¨åˆ†ã®å‰²åˆãŒå¤§ãã‘ã‚Œã°ãã®äºŒã¤ã®æ–‡æ›¸ã¯ä¼¼ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqr10Mw-K5UQ"
      },
      "source": [
        "def jaccard_similarity(set_a,set_b):\n",
        "  # ç©é›†åˆã®è¦ç´ æ•°ã‚’è¨ˆç®—\n",
        "  num_intersection = len(set.intersection(set_a, set_b))\n",
        "  # å’Œé›†åˆã®è¦ç´ æ•°ã‚’è¨ˆç®—\n",
        "  num_union = len(set.union(set_a, set_b))\n",
        "  #Jaccardä¿‚æ•°ã‚’ç®—å‡ºã€€ç©ºé›†åˆã®æ™‚ã¯1ã‚’å‡ºåŠ›\n",
        "  try:\n",
        "      return float(num_intersection) / num_union\n",
        "  except ZeroDivisionError:\n",
        "      return 1.0 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZSFY5urK8eT",
        "outputId": "dd9c93bc-df41-4bce-eb2d-11bdaf7ea098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"jaccard(a, b) = \", jaccard_similarity(set_a, set_b)) #Jaccardä¿‚æ•°ã‚’è¨ˆç®—\n",
        "print(\"jaccard(a, c) = \", jaccard_similarity(set_a, set_c))\n",
        "print(\"jaccard(b, c) = \", jaccard_similarity(set_b, set_c))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard(a, b) =  0.5714285714285714\n",
            "jaccard(a, c) =  0.11764705882352941\n",
            "jaccard(b, c) =  0.05555555555555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXBk5SiTMmGf"
      },
      "source": [
        "\n",
        "nltkã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹  \n",
        "å®šç¾©ã¨åŒã˜ã‚ˆã†ã«è¨ˆç®—ã‚’è¡Œã†ã®ã§ã€å…¥åŠ›ã¯é›†åˆ  \n",
        "è·é›¢ã«ãªã£ã¦ã„ã‚‹ã¨ã“ã‚ã«ã¯æ³¨æ„ãŒå¿…è¦"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZQ9q8mFLJTO",
        "outputId": "3333bb6d-0ab3-4865-db8f-9835f2011b86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.metrics import jaccard_distance\n",
        "\n",
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "# Jaccardè·é›¢ã«ãªã£ã¦ã„ã‚‹ã®ã§ã€é¡ä¼¼åº¦ã«å¤‰æ›ã™ã‚‹ã¨ãã¯1ã‹ã‚‰å¼•ã\n",
        "print(\"jaccard(a, b) = \", 1 - jaccard_distance(set_a, set_b))\n",
        "print(\"jaccard(a, c) = \", 1 - jaccard_distance(set_a, set_c))\n",
        "print(\"jaccard(b, c) = \", 1 - jaccard_distance(set_b, set_c))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard(a, b) =  0.5714285714285714\n",
            "jaccard(a, c) =  0.11764705882352944\n",
            "jaccard(b, c) =  0.05555555555555558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lyCVTfePwB2"
      },
      "source": [
        "#### SÃ¸rensen-Diceä¿‚æ•°\n",
        "\n",
        "Jaccardä¿‚æ•°ã§ã¯åˆ†æ¯ã¯ã®å’Œé›†åˆã§ã‚ã£ãŸãŸã‚  \n",
        "ç‰‡æ–¹ã®é›†åˆãŒã¨ã¦ã‚‚å¤§ãã„ã¨å…±é€šéƒ¨åˆ†ãŒå¤§ããã¦ã‚‚ä¿‚æ•°ã®å€¤ãŒå°ã•ããªã£ã¦ã—ã¾ã†ã¨ã„ã†å•é¡ŒãŒã‚ã‚‹  \n",
        "SÃ¸rensen-Diceä¿‚æ•°ã§ã¯ã€åˆ†æ¯ã‚’äºŒã¤ã®é›†åˆã®å¤§ãã•ã®å¹³å‡ã‚’ã¨ã‚‹ã“ã¨ã§ã€ãã®å½±éŸ¿ã‚’ç·©å’Œã—ã¦ã„ã‚‹  \n",
        "\n",
        "$\n",
        "DSC(A,B) = \\dfrac{|A\\cap B|}{\\dfrac{|A| + |B|}{2}} = \\dfrac{2|A\\cap B|}{|A| + |B|}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M0RikFPR3Qr"
      },
      "source": [
        "def dice_similarity(set_a, set_b):\n",
        "  num_intersection =  len(set.intersection(set_a, set_b))\n",
        "  sum_nums = len(set_a) + len(set_b)\n",
        "  try:\n",
        "    return 2 * num_intersection / sum_nums\n",
        "  except ZeroDivisionError:\n",
        "    return 1.0 "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZQFbXlESPWl",
        "outputId": "cf9b84ba-54a6-4457-e10b-d416537a0de2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"dice(a, b) = \", dice_similarity(set_a, set_b))\n",
        "print(\"dice(a, c) = \", dice_similarity(set_a, set_c))\n",
        "print(\"dice(b, c) = \", dice_similarity(set_b, set_c))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dice(a, b) =  0.7272727272727273\n",
            "dice(a, c) =  0.21052631578947367\n",
            "dice(b, c) =  0.10526315789473684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxtliI-HPwRE"
      },
      "source": [
        "#### Szymkiewicz-Simpsonä¿‚æ•°\n",
        "\n",
        "å·®é›†åˆã®è¦ç´ æ•°ã®å½±éŸ¿ã‚’æ¥µé™ã¾ã§æŠ‘ãˆãŸã®ãŒSzymkiewicz-Simpsonä¿‚æ•°    \n",
        "$\n",
        "overlap(ğ´,ğµ) = \\dfrac{|A\\cap B|}{\\min(|A|, |B|)}\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgGJ5GhmUduH"
      },
      "source": [
        "def simpson_similarity(list_a, list_b):\n",
        "  num_intersection = len(set.intersection(set(list_a), set(list_b)))\n",
        "  min_num = min(len(set(list_a)), len(set(list_b)))\n",
        "  try:\n",
        "    return num_intersection / min_num\n",
        "  except ZeroDivisionError:\n",
        "    if num_intersection == 0:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_1Gjy4IV9eo",
        "outputId": "309b6c29-1d91-47c7-931e-51c6e53ecdd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"simpson(a, b) = \", simpson_similarity(set_a, set_b)) \n",
        "print(\"simpson(a, c) = \", simpson_similarity(set_a, set_c)) \n",
        "print(\"simpson(b, c) = \", simpson_similarity(set_b, set_c)) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simpson(a, b) =  0.7272727272727273\n",
            "simpson(a, c) =  0.25\n",
            "simpson(b, c) =  0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-T32gmnZIyt"
      },
      "source": [
        "#### Exercise 1\n",
        "è‰²ã€…ãªé›†åˆã‚’ä½œã£ã¦é›†åˆãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã®æ¯”è¼ƒã‚’ã—ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJwJpZpdYtz9",
        "outputId": "e19e4ff8-baac-4000-aa93-4deb1613f224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "set_d = set() # å¤§ãã‚ã®é›†åˆã‚’ä½œã£ã¦è©¦ã—ã¦ã¿ã‚ˆã†\n",
        "\n",
        "print(\"jaccard similarity:\")\n",
        "print(jaccard_similarity(set_d, set_a))\n",
        "print(jaccard_similarity(set_d, set_b))\n",
        "print(jaccard_similarity(set_d, set_c))\n",
        "\n",
        "print(\"dice similarity:\")\n",
        "print(dice_similarity(set_d, set_a))\n",
        "print(dice_similarity(set_d, set_b))\n",
        "print(dice_similarity(set_d, set_c))\n",
        "\n",
        "print(\"simpson similarity:\")\n",
        "print(simpson_similarity(set_d, set_a))\n",
        "print(simpson_similarity(set_d, set_b))\n",
        "print(simpson_similarity(set_d, set_c))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard similarity:\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "dice similarity:\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "simpson similarity:\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Hm34tgXCbh"
      },
      "source": [
        "### ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ \n",
        "\n",
        "\n",
        "æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ç¾ã—é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹  \n",
        "ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®æ‰‹æ³•ã¯è‰²ã€…ã‚ã‚‹ãŒä»Šå›ã¯BoW(Bag of Words)ã§èª¬æ˜ã™ã‚‹  \n",
        "\n",
        "BoWã¯æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã™ã‚‹æ–¹æ³•ã®ä¸€ã¤  \n",
        "æƒ³å®šã—ã¦ã„ã‚‹å˜èªã®ç·æ•°ã‚’Nã¨ã™ã‚‹ã¨ã€å„æ¬¡å…ƒãŒå„å˜èªã«å¯¾å¿œã™ã‚‹Næ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è€ƒãˆã‚‹  \n",
        "å„æ¬¡å…ƒã®å€¤ã¯ãã®å˜èªãŒæ–‡æ›¸ä¸­ã§å‡ºãŸå›æ•°\n",
        "\n",
        "ä¾‹ï¼‰  \n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "â†“  \n",
        "å…¨å˜èªã¯19å€‹ã§ã€å„æ¬¡å…ƒã®å€¤ã¯ä»¥ä¸‹ã®å˜èªã®å€‹æ•°ã«å¯¾å¿œã™ã‚‹BoWã‚’è€ƒãˆã‚‹  \n",
        "['an', 'and', 'apple', 'apples', 'basketball', 'bought', 'buy', 'day', 'eat', 'every', 'i', 'jordan', 'like', 'michael', 'play', 'some', 'strawberries', 'tomorrow', 'will']  \n",
        "â†“  \n",
        "BoW A : [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]  \n",
        "BoW B : [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
        "BoW C : [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
        "\n",
        "ã“ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’è¡¨ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQOjHTfCeXjs"
      },
      "source": [
        "\n",
        "#### ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢\n",
        "\n",
        "å„æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ã™ã“ã¨ãŒå‡ºæ¥ãŸã®ã§  \n",
        "ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ãŒè¨ˆç®—ã§ãã‚‹  \n",
        "ã“ã®è·é›¢ãŒå°ã•ã‘ã‚Œã°ä¼¼ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹\n",
        "\n",
        "\\begin{equation}\n",
        "d(v_1,v_2) =(\\sum_{i=1}^n (v_{1i}-v_{2i})^2)^{\\frac{1}{2}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtEzuSDKZbpV"
      },
      "source": [
        "def euclidean_distance(list_a, list_b):\n",
        "  diff_vec = np.array(list_a) - np.array(list_b)\n",
        "  return np.linalg.norm(diff_vec)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sr_ZYj7azLV",
        "outputId": "4af5e8a0-3711-4749-f714-d3af7b05e45a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bow_a = [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]  \n",
        "bow_b = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
        "bow_c = [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
        "\n",
        "print(\"euclidean_distance(bow_a, bow_b) = \",euclidean_distance(bow_a, bow_b))\n",
        "print(\"euclidean_distance(bow_a, bow_c) = \",euclidean_distance(bow_a, bow_c))\n",
        "print(\"euclidean_distance(bow_b, bow_c) = \",euclidean_distance(bow_b, bow_c))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "euclidean_distance(bow_a, bow_b) =  2.23606797749979\n",
            "euclidean_distance(bow_a, bow_c) =  3.7416573867739413\n",
            "euclidean_distance(bow_b, bow_c) =  4.123105625617661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfKcQd5SiBb-"
      },
      "source": [
        "#### ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼è·é›¢\n",
        "\n",
        "ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’ä¸€èˆ¬åŒ–ã—ãŸè·é›¢\n",
        "pã®å€¤ã‚’å¤‰ãˆã‚‹ã“ã¨ã§è‰²ã€…ãªè·é›¢ã‚’è¡¨ç¾ã§ãã‚‹  \n",
        "\n",
        "\\begin{equation}\n",
        "d(v_1,v_2) = (\\sum_{i=1}^n |v_{1i}-v_{2i}|^p)^{\\frac{1}{p}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx46KLSNhjI2"
      },
      "source": [
        "#### Exercise 2\n",
        "ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼è·é›¢ã‚’è¨ˆç®—ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã„ã¦  \n",
        "p=1,2,3ã§è·é›¢ã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6UMFh58bexH"
      },
      "source": [
        "# np.linalg.normã«ã¤ã„ã¦èª¿ã¹ã‚ˆã†\n",
        "def minkowski_distance(list_a, list_b, p):\n",
        "  ## return 0ã¯ãƒ€ãƒŸãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚å‰Šé™¤ã—ã¦\n",
        "  ##ã€€è‡ªåˆ†ã§ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã¦ãã ã•ã„ã€‚\n",
        "  return 0"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKMIMbyJc8eH"
      },
      "source": [
        "# p=1\n",
        "print(minkowski_distance(bow_a, bow_b, 1))\n",
        "print(minkowski_distance(bow_a, bow_c, 1))\n",
        "print(minkowski_distance(bow_b, bow_c, 1))\n",
        "\n",
        "# p=2\n",
        "print(minkowski_distance(bow_a, bow_b, 2))\n",
        "print(minkowski_distance(bow_a, bow_c, 2))\n",
        "print(minkowski_distance(bow_b, bow_c, 2))\n",
        "\n",
        "# p=3\n",
        "print(minkowski_distance(bow_a, bow_b, 3))\n",
        "print(minkowski_distance(bow_a, bow_c, 3))\n",
        "print(minkowski_distance(bow_b, bow_c, 3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIyabGRTKGwA"
      },
      "source": [
        "#### ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
        "\n",
        "ãƒ™ã‚¯ãƒˆãƒ«ã®ãªã™è§’ã«ç€ç›®ã—ã¦é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹  \n",
        "\n",
        "\\begin{equation}\n",
        "similarity(A, B)=cos(\\theta)=\\dfrac{\\sum_{i=1}^n A_iB_i}{{\\sqrt A}{\\sqrt B}}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWy619amlwp"
      },
      "source": [
        "#### Exercise 3\n",
        "ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã„ã¦è¨ˆç®—ã—ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDhwqTN5yJGQ"
      },
      "source": [
        "# numpy.array ã«ã¤ã„ã¦èª¿ã¹ã‚ˆã†\n",
        "def cosine_similarity(list_a, list_b):\n",
        "  # ã‚ã¨ã§æ¶ˆã™\n",
        "  inner_prod = np.array(list_a).dot(np.array(list_b))\n",
        "  norm_a = np.linalg.norm(list_a)\n",
        "  norm_b = np.linalg.norm(list_b)\n",
        "  try:\n",
        "      return inner_prod / (norm_a*norm_b)\n",
        "  except ZeroDivisionError:\n",
        "      return 1.0"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sew3u-YezRrX",
        "outputId": "8bf38eb0-5a79-4c78-81e8-72cc2b6f8f68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bow_a = [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]\n",
        "bow_b = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]\n",
        "bow_c = [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "print(\"cosine_similarity(bow_a, bow_b) = \",cosine_similarity(bow_a, bow_b))\n",
        "print(\"cosine_similarity(bow_a, bow_c) = \",cosine_similarity(bow_a, bow_c))\n",
        "print(\"cosine_similarity(bow_b, bow_c) = \",cosine_similarity(bow_b, bow_c))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine_similarity(bow_a, bow_b) =  0.8153742483272114\n",
            "cosine_similarity(bow_a, bow_c) =  0.41812100500354543\n",
            "cosine_similarity(bow_b, bow_c) =  0.3223291856101521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao0jywXqKIFS"
      },
      "source": [
        "### é›†åˆãƒ™ãƒ¼ã‚¹ã¨ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®æ¯”è¼ƒ\n",
        "\n",
        "é›†åˆæ¼”ç®—ã®æ–¹ã¯ä¸€ã¤ä¸€ã¤ã®æ–‡æ›¸ãŒå°ã•ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ€§èƒ½ãŒé«˜ã„  \n",
        "æ–‡æ›¸ãŒã‚ã‚‹ç¨‹åº¦å¤§ãããªã‚‹ã¨ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®æ–¹ãŒæœ‰ç”¨ã«ãªã‚‹  \n",
        "ãã®ä»£ã‚ã‚Šã€èªå½™é›†åˆãŒå¤§ãããªã‚Šè¨ˆç®—é‡ãŒå¤§ãããªã£ã¦ã—ã¾ã†\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1IXMwIyhU-6"
      },
      "source": [
        "### Exercise 4\n",
        "çŸ­ã„æ–‡ç« ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨é•·ã„æ–‡ç« ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªåˆ†ã§ä½œã‚Š    \n",
        "Jaccardä¿‚æ•°ã¨ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦æ¯”è¼ƒã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpkcXVDBhZg8"
      },
      "source": [
        "short_docs = []\n",
        "long_docs = []"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuXSHk01KLza"
      },
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "é›†åˆé–“ã®å…±é€šéƒ¨åˆ†ã‚„ãƒ™ã‚¯ãƒˆãƒ«é–“ã®è·é›¢ã‚„è§’åº¦ã§é¡ä¼¼åº¦ã‚’æ¸¬ã‚‹ã“ã¨ãŒå‡ºæ¥ãŸ  \n",
        "é›†åˆã‚„ãƒ™ã‚¯ãƒˆãƒ«ãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’ä¸Šæ‰‹ãè¡¨ã›ã¦ã„ãªã„ã¨é¡ä¼¼åº¦ãŒä¸Šæ‰‹ãæ¸¬ã‚Œãªã„  \n",
        "æ–‡æ›¸ã‹ã‚‰ã©ã®ã‚ˆã†ã«é›†åˆã‚„ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œã‚‹ã‹ãŒã¨ã¦ã‚‚å¤§äº‹  \n",
        " \n",
        "é©åˆ‡ãªå‰å‡¦ç†ã‚’è¡Œã†ã“ã¨ã§ç‰¹å¾´ã‚’æ‰ãˆãŸé¡ä¼¼åº¦ã‚’æ¸¬ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹    \n",
        "å¾ŒåŠã¯ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«çµã£ã¦ç·´ç¿’ã—ã¦ã„ã  \n",
        "\n",
        "1. Clearning\n",
        "2. Tokenize\n",
        "3. Stemming\n",
        "4. Remove stop words\n",
        "5. Vectorize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4ispTSLKM-z"
      },
      "source": [
        "### 2-1. Clearning\n",
        "\n",
        "ä¸Šã®ä¾‹ã§ã¯ç¶ºéº—ãªæ–‡ç« ã°ã‹ã‚Šæ‰±ã£ã¦ã„ãŸãŒã€å®Ÿéš›ã¯ã‚‚ã£ã¨æ±šã„   \n",
        "Webã‹ã‚‰å–ã£ã¦ããŸãƒ‡ãƒ¼ã‚¿ã ã¨htmlã‚¿ã‚°ãŒæ®‹ã£ã¦ã„ãŸã‚Šã€å¤‰ãªè¨˜å·ãŒå…¥ã£ã¦ã„ãŸã‚Šã™ã‚‹  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM-e5AZT316r"
      },
      "source": [
        "documents=[\"I like apples and a strawberries. I will buy an apple tomorrow @Fresco.\",\n",
        "           \"I bought some apples and strawberries. I will eat an apple <b>tomorrow.</b>\",\n",
        "           \"I play basketball every day. I like Michael Jordan (born February 17, 1963).\"]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh56eU_levzv"
      },
      "source": [
        "ä»Šã¯ä¸‰ã¤ãªã®ã§æ‰‹å‹•ã§æ¶ˆã›ã‚‹ãŒ  \n",
        "å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã¨ãã«ã¯è‡ªå‹•ã§ç¶ºéº—ã«ã§ããªã„ã¨ã„ã‘ãªã„  \n",
        "ç¶ºéº—ã«ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œã‚‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06coot4PnVgS"
      },
      "source": [
        "#### Exercise 5\n",
        "\n",
        "æ­£è¦è¡¨ç¾ã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¶ºéº—ã«ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã“ã†\n",
        "\n",
        "å‚è€ƒ: æ­£è¦è¡¨ç¾ (https://uxmilk.jp/41416)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt3NL8Ux5Q4E",
        "outputId": "c058f8b0-a75a-4f62-f449-6c72fbc98841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "\n",
        "def cleaning_text(text):\n",
        "    # @ã®å‰Šé™¤\n",
        "    pattern1 = '@'\n",
        "    text = re.sub(pattern1, '', text)    \n",
        "    # <b>ã‚¿ã‚°ã®å‰Šé™¤ã€€\n",
        "    # æ¬¡ã®è¡Œã®'#'ã‚’å‰Šé™¤ã—ã¦ã€ã‚³ãƒ¼ãƒ‰ã‚’è£œå®Œã—ã¦ãã ã•ã„ã€‚\n",
        "    pattern2 = # \n",
        "    text = re.sub(pattern2, '', text)    \n",
        "    # ()å†…ã‚’å‰Šé™¤\n",
        "    # æ¬¡ã®è¡Œã®'#'ã‚’å‰Šé™¤ã—ã¦ã€ã‚³ãƒ¼ãƒ‰ã‚’è£œå®Œã—ã¦ãã ã•ã„ã€‚\n",
        "    pattern3 = #\n",
        "    text = re.sub(pattern3, '', text)\n",
        "    return text\n",
        "  \n",
        "\n",
        "for text in documents:\n",
        "    print(cleaning_text(text))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like apples and a strawberries. I will buy an apple tomorrow Fresco.\n",
            "I bought some apples and strawberries. I will eat an apple tomorrow.\n",
            "I play basketball every day. I like Michael Jordan .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcgcKWkRflR5"
      },
      "source": [
        "#### Option 1\n",
        "\n",
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¶ºéº—ã«ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã‚ˆã†\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsdHgV-h26y"
      },
      "source": [
        "text = '<p><b>Natural language processing</b> (<b>NLP</b>) is a subfield of <a href=\"/wiki/Computer_science\" title=\"Computer science\">computer science</a>, <a href=\"/wiki/Information_engineering_(field)\" title=\"Information engineering (field)\">information engineering</a>, and <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of <a href=\"/wiki/Natural_language\" title=\"Natural language\">natural language</a> data.</p>'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1doxCMAGr0_O"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edJ80nTbQgLi"
      },
      "source": [
        "\n",
        "### 2-2. Tokenize\n",
        "\n",
        "ã¾ã æ–‡å­—åˆ—ã®ã¾ã¾ãªã®ã§ã€å˜èªã”ã¨ã«åŒºåˆ‡ã‚‹  \n",
        "è‹±èªã ã¨ç©ºç™½åŒºåˆ‡ã‚Šã§ã‚ˆã„ãŒæ—¥æœ¬èªã ã¨å°‘ã—é¢å€’  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGbqTvkyYl2S",
        "outputId": "171d8130-7be6-4e96-a0a9-94292a999666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def tokenize_text(text):\n",
        "  text = re.sub('[.,]', '', text)\n",
        "  return text.split()\n",
        "\n",
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  print(tokenize_text(text))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'like', 'apples', 'and', 'a', 'strawberries', 'I', 'will', 'buy', 'an', 'apple', 'tomorrow', 'Fresco']\n",
            "['I', 'bought', 'some', 'apples', 'and', 'strawberries', 'I', 'will', 'eat', 'an', 'apple', 'tomorrow']\n",
            "['I', 'play', 'basketball', 'every', 'day', 'I', 'like', 'Michael', 'Jordan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K97zCvZBQik9"
      },
      "source": [
        "### 2-3. Stemming, Lemmatize\n",
        "\n",
        "åŒã˜æ„å‘³ã®å˜èªã§ã‚‚ç•°ãªã‚‹å½¢ã‚’ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹  \n",
        "ãã‚Œã‚‰ã‚’åˆ¥ã®å˜èªã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã®ã¯ä¸è‡ªç„¶  \n",
        "å°æ–‡å­—ã«å¤‰æ›ã—ãŸå¾Œ  \n",
        "Stemmingã‚„Lemmatizeã¨ã„ã†å‡¦ç†ã§åŒã˜å½¢ã«ã™ã‚‹  \n",
        "ä»Šå›ã¯Lemmatizeã®ã¿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7R55rKxZtAz"
      },
      "source": [
        "from nltk.corpus import wordnet as wn #lemmatizeé–¢æ•°ã®ãŸã‚ã®import\n",
        "\n",
        "def lemmatize_word(word):\n",
        "    # make words lower  example: Python =>python\n",
        "    word=word.lower()\n",
        "    \n",
        "    # lemmatize  example: cooked=>cook\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "      return lemma"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0sEVvmwaCqA",
        "outputId": "c71443cd-4542-428d-f0cf-c5e9511195a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  print([lemmatize_word(word) for word in tokens])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'like', 'apple', 'and', 'a', 'strawberry', 'i', 'will', 'buy', 'an', 'apple', 'tomorrow', 'fresco']\n",
            "['i', 'buy', 'some', 'apple', 'and', 'strawberry', 'i', 'will', 'eat', 'an', 'apple', 'tomorrow']\n",
            "['i', 'play', 'basketball', 'every', 'day', 'i', 'like', 'michael', 'jordan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPIFqvjfxEg"
      },
      "source": [
        "strawberriesâ†’strawberryã®ã‚ˆã†ã«èªã‚’æ¨™æº–å½¢ã«å¤‰æ›å‡ºæ¥ãŸ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpUxnB9kQlKv"
      },
      "source": [
        "### 2-4. Remove stop words\n",
        "\n",
        "a, theãªã©ã®æ–‡ç« ã«å¯„ã‚‰ãšä¸€èˆ¬çš„ã«ä½¿ã‚ã‚Œã‚‹å† è©ã€ä»£åè©ã€å‰ç½®è©ãªã©ã‚’ä½¿ã£ã¦ã‚‚æ„å‘³ãŒãªã„  \n",
        "ãã‚Œã‚‰ã®å˜èªã¯stop wordã¨å‘¼ã°ã‚Œã‚‹  \n",
        "nltkã«ã¯å°‚é–€å®¶ãŒå®šç¾©ã—ãŸstop wordã®ãƒªã‚¹ãƒˆãŒã‚ã‚‹ã®ã§ãã‚Œã‚’ä½¿ã†  \n",
        "å¿…è¦ã«å¿œã˜ã¦stop wordã¯è‡ªåˆ†ã§ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ã¹ã  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbSLDay-a36N",
        "outputId": "403d24ce-dcfb-463a-d8f7-820adb392d28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#1 nltkã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ\n",
        "en_stop = nltk.corpus.stopwords.words('english')\n",
        "print(en_stop)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX8eIyxfbo6q"
      },
      "source": [
        "def remove_stopwords(word, stopwordset):\n",
        "  if word in stopwordset:\n",
        "    return None\n",
        "  else:\n",
        "    return word"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WckIX0ThbQMy",
        "outputId": "e14f89ac-2b08-48aa-b7c3-624207604bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  print([remove_stopwords(word, en_stop) for word in tokens])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 'like', 'apple', None, None, 'strawberry', None, None, 'buy', None, 'apple', 'tomorrow', 'fresco']\n",
            "[None, 'buy', None, 'apple', None, 'strawberry', None, None, 'eat', None, 'apple', 'tomorrow']\n",
            "[None, 'play', 'basketball', 'every', 'day', None, 'like', 'michael', 'jordan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4kNHhr5f8Vt"
      },
      "source": [
        "ä»Šå›ã¯ã“ã‚Œã ã‘ã§çµ‚ã‚ã‚Šã«ã™ã‚‹ãŒå˜èªã®å‰Šé™¤ã¯ã‹ãªã‚Šé‡è¦  \n",
        "å‡ºç¾é »åº¦ãŒæ¥µç«¯ã«ä½ã„å˜èªã‚’å‰Šé™¤ã—ãŸã‚Šã€å‹•è©ã¨åè©ã«é™å®šã™ã‚‹ãªã©è‰²ã€…ã‚ã‚‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgjfOITFxwx_",
        "outputId": "284a0284-9c89-43c5-ebc6-45adc0fbb3c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def preprocessing_text(text):\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
        "  tokens = [word for word in tokens if word is not None]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "preprocessed_docs = [preprocessing_text(text) for text in documents]\n",
        "preprocessed_docs"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['like', 'apple', 'strawberry', 'buy', 'apple', 'tomorrow', 'fresco'],\n",
              " ['buy', 'apple', 'strawberry', 'eat', 'apple', 'tomorrow'],\n",
              " ['play', 'basketball', 'every', 'day', 'like', 'michael', 'jordan']]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCd6YpYbKJSJ"
      },
      "source": [
        "### 2-5. Vectorize\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmrBtgPxcwX0"
      },
      "source": [
        "#### BoW(Bag of Words)\n",
        "\n",
        "\n",
        "ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã®å‡ºç¾å›æ•°ã®ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ã—ãŸã‚‚ã®  \n",
        "äººæ‰‹ã§å˜èªã‚’æ•°ãˆãŸã‚Šã™ã‚‹ã®ã¯ä¸å¯èƒ½ãªã®ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å‡¦ç†ã‚’å®Œçµã—ã¦ã—ã¾ãŠã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S69MsPjHcvut"
      },
      "source": [
        "def bow_vectorizer(docs):\n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "        \n",
        "  result_list = []\n",
        "  for doc in docs:\n",
        "    doc_vec = [0] * len(word2id)\n",
        "    for w in doc:\n",
        "      doc_vec[word2id[w]] += 1\n",
        "    result_list.append(doc_vec)\n",
        "  return result_list, word2id"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq3TpP7KO-XP",
        "outputId": "6c1f8b2e-d8f8-4796-897d-b1cd389ba4c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bow_vec, word2id = bow_vectorizer(preprocessed_docs)\n",
        "print(bow_vec)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udTTbKv8oFpG"
      },
      "source": [
        "word2id.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e8h1SMTcyD5"
      },
      "source": [
        "### TF-IDF(Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "BoWã§ã¯å„å˜èªã®é‡ã¿ãŒåŒã˜ã ã£ãŸãŒã€å˜èªã«ã‚ˆã£ã¦é‡è¦åº¦ã¯å¤‰ã‚ã‚‹  \n",
        "å˜èªã®é‡è¦åº¦ã‚’è€ƒæ…®ã—ãŸã®ãŒTF-IDF  \n",
        "\n",
        "TF(t, d) = ã‚ã‚‹å˜èª(t)ã®ã‚ã‚‹æ–‡æ›¸(d)ã«ãŠã‘ã‚‹å‡ºç¾é »åº¦  \n",
        "IDF(t) = ã‚ã‚‹å˜èª(t)ãŒå…¨æ–‡æ›¸é›†åˆ(D)ä¸­ã«ã©ã‚Œã ã‘ã®æ–‡æ›¸ã§å‡ºç¾ã—ãŸã‹ã®é€†æ•°  \n",
        "\n",
        "TF-IDF(t,d) = TF(t, d) * IDF(t)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiKn2p7Bc0bN"
      },
      "source": [
        "def tfidf_vectorizer(docs):\n",
        "  def tf(word2id, doc):\n",
        "    term_counts = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      term_counts[word2id[term]] = doc.count(term)\n",
        "    tf_values = list(map(lambda x: x/sum(term_counts), term_counts))\n",
        "    return tf_values\n",
        "  \n",
        "  def idf(word2id, docs):\n",
        "    idf = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      idf[word2id[term]] = np.log(len(docs) / sum([bool(term in doc) for doc in docs]))\n",
        "    return idf\n",
        "  \n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "  \n",
        "  return [[_tf*_idf for _tf, _idf in zip(tf(word2id, doc), idf(word2id, docs))] for doc in docs], word2id\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz5YnQZclGfL"
      },
      "source": [
        "tfidf_vector, word2id = tfidf_vectorizer(preprocessed_docs)\n",
        "print(tfidf_vector)\n",
        "print(word2id.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxCJEOjXYSIP"
      },
      "source": [
        "### Exercise 6\n",
        "BoWã¨TF-IDFã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ãã‚Œãã‚Œè¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD4nID08s-21"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAUfDUOmJMsp"
      },
      "source": [
        "### Option 2\n",
        "scikit-learn, nltk gensimãã‚Œãã‚Œã«TF-IDFã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ãŒã‚ã‚‹  \n",
        "ãã‚Œãã‚Œã§TF-IDFã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2tZLjHNxRDG"
      },
      "source": [
        "# scikit-learnã®tfidfã€€ã‚ã¨ã§æ¶ˆã™\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x7nd2-ekIs-",
        "outputId": "95e7002f-dbb2-4414-ecc2-6e10b474b8b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#nltk ã®tf-idfã€€ã‚ã¨ã§æ¶ˆã™\n",
        "collection = nltk.TextCollection(preprocessed_docs)\n",
        "terms = list(set(collection))\n",
        "nltk_vector = []\n",
        "for doc in preprocessed_docs:\n",
        "  tmp_vec = np.zeros(len(word2id))\n",
        "  for term in word2id.keys():\n",
        "    tmp_vec[word2id[term]] = collection.tf_idf(term, doc)\n",
        "  nltk_vector.append(list(tmp_vec))\n",
        "print(nltk_vector)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.05792358687259491, 0.11584717374518982, 0.05792358687259491, 0.05792358687259491, 0.05792358687259491, 0.15694461266687282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.13515503603605478, 0.06757751801802739, 0.06757751801802739, 0.06757751801802739, 0.0, 0.1831020481113516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05792358687259491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15694461266687282, 0.15694461266687282, 0.15694461266687282, 0.15694461266687282, 0.15694461266687282, 0.15694461266687282]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEyOY2eQkRMc",
        "outputId": "be4baaec-51b7-4d3f-b76c-214c3eeb8807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#gensim tf-idf ã‚ã¨ã§æ¶ˆã™\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_docs)\n",
        "print('===å˜èª->idã®å¤‰æ›è¾æ›¸===')\n",
        "print(dictionary.token2id)\n",
        "print(word2id)\n",
        "\n",
        "corpus = list(map(dictionary.doc2bow, preprocessed_docs))\n",
        "test_model = models.TfidfModel(corpus)\n",
        "corpus_tfidf = test_model[corpus]\n",
        "\n",
        "print('===çµæœè¡¨ç¤º===')\n",
        "gensim_vector = []\n",
        "for doc in corpus_tfidf:\n",
        "  tmp_vec = [0] * len(word2id)\n",
        "  for word in doc:\n",
        "    key = dictionary[word[0]]\n",
        "    tmp_vec[word2id[key]] = word[1]\n",
        "  gensim_vector.append(tmp_vec)\n",
        "\n",
        "print(gensim_vector)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===å˜èª->idã®å¤‰æ›è¾æ›¸===\n",
            "{'apple': 0, 'buy': 1, 'fresco': 2, 'like': 3, 'strawberry': 4, 'tomorrow': 5, 'eat': 6, 'basketball': 7, 'day': 8, 'every': 9, 'jordan': 10, 'michael': 11, 'play': 12}\n",
            "{'like': 0, 'apple': 1, 'strawberry': 2, 'buy': 3, 'tomorrow': 4, 'fresco': 5, 'eat': 6, 'play': 7, 'basketball': 8, 'every': 9, 'day': 10, 'michael': 11, 'jordan': 12}\n",
            "===çµæœè¡¨ç¤º===\n",
            "[[0.25530938246616874, 0.5106187649323375, 0.25530938246616874, 0.25530938246616874, 0.25530938246616874, 0.6917636545800514, 0, 0, 0, 0, 0, 0, 0], [0, 0.528121006538623, 0.2640605032693115, 0.2640605032693115, 0.2640605032693115, 0, 0.7154749152081473, 0, 0, 0, 0, 0, 0], [0.1489905855640844, 0, 0, 0, 0, 0, 0, 0.40369167389095173, 0.40369167389095173, 0.40369167389095173, 0.40369167389095173, 0.40369167389095173, 0.40369167389095173]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMjo7F6ZKOW1"
      },
      "source": [
        "## Exercise 7\n",
        "\n",
        "æ§˜ã€…ãªå›½ã®Wikipediaã«ãŠã‘ã‚‹abstractã‚’å–ã‚Šå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã—ãŸ  \n",
        "https://drive.google.com/open?id=1i7tekPQRKaAwg-ze3kv5IsufMW13LkLo  \n",
        "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ã†  \n",
        "\n",
        "Cosineé¡ä¼¼åº¦ã®è¨ˆç®—ã‚’è¡Œã„ã€Japanã«ä¼¼ã¦ã„ã‚‹å›½Top5ã‚’è¡¨ç¤ºã—ã¦ã¿ã‚ˆã†  \n",
        "å‰å‡¦ç†ã‚’è‡ªåˆ†ãªã‚Šã«å·¥å¤«ã™ã‚‹ã“ã¨  \n",
        "æ³¨ï¼‰é¡ä¼¼åº¦ã¯ã‚ã¾ã‚Šé«˜ããªã‚‰ãªãã¦ã‚‚è‰¯ã„  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bDcIqK4Yc9_l",
        "outputId": "2357e309-ddbb-47a3-88a3-d4d6f226bb01"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1IbA30KKElT",
        "outputId": "2c5f06e8-9070-4aa3-9bbc-47fba80ec564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "source": [
        "df = pd.read_csv(\"./nlp_country.csv\")\n",
        "df"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Name                                           Abstract\n",
              "0           Japan  Japan is an island country in East Asia. Locat...\n",
              "1   United States  The United States of America (USA), commonly k...\n",
              "2         England  England is a country that is part of the Unite...\n",
              "3           China  China, officially the People's Republic of Chi...\n",
              "4           India  India, also known as the Republic of India,[19...\n",
              "5           Korea  Korea is a region in East Asia.[3] Since 1948 ...\n",
              "6         Germany  Germany, officially the Federal Republic of Ge...\n",
              "7          Russia  Russia, or the Russian Federation[12], is a tr...\n",
              "8          France  France, officially the French Republic, is a c...\n",
              "9           Italy  Italy, officially the Italian Republic,[10][11...\n",
              "10         Brazil  Brazil officially the Federative Republic of B...\n",
              "11         Canada  Canada is a country in the northern part of No...\n",
              "12          Spain  Spain, officially the Kingdom of Spain[11][a][...\n",
              "13      Australia  Australia, officially the Commonwealth of Aust...\n",
              "14      Indonesia  Indonesia, officially the Republic of Indonesi...\n",
              "15         Mexico  Mexico, officially the United Mexican States (..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-331b8394-f1ba-47e6-8410-686470c84939\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Japan is an island country in East Asia. Locat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>United States</td>\n",
              "      <td>The United States of America (USA), commonly k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>England</td>\n",
              "      <td>England is a country that is part of the Unite...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>China</td>\n",
              "      <td>China, officially the People's Republic of Chi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>India</td>\n",
              "      <td>India, also known as the Republic of India,[19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Korea</td>\n",
              "      <td>Korea is a region in East Asia.[3] Since 1948 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Germany</td>\n",
              "      <td>Germany, officially the Federal Republic of Ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Russia</td>\n",
              "      <td>Russia, or the Russian Federation[12], is a tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>France</td>\n",
              "      <td>France, officially the French Republic, is a c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Italy</td>\n",
              "      <td>Italy, officially the Italian Republic,[10][11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Brazil</td>\n",
              "      <td>Brazil officially the Federative Republic of B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Canada</td>\n",
              "      <td>Canada is a country in the northern part of No...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Spain</td>\n",
              "      <td>Spain, officially the Kingdom of Spain[11][a][...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Australia</td>\n",
              "      <td>Australia, officially the Commonwealth of Aust...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Indonesia, officially the Republic of Indonesi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Mexico</td>\n",
              "      <td>Mexico, officially the United Mexican States (...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-331b8394-f1ba-47e6-8410-686470c84939')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-331b8394-f1ba-47e6-8410-686470c84939 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-331b8394-f1ba-47e6-8410-686470c84939');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2ZqrWJhBAO2",
        "outputId": "420d8537-ff1c-497a-bcbe-b187640ce49e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "df.iloc[0][\"Abstract\"]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Japan is an island country in East Asia. Located in the Pacific Ocean, it lies off the eastern coast of the Asian continent and stretches from the Sea of Okhotsk in the north to the East China Sea and the Philippine Sea in the south. The kanji that make up Japan\\'s name mean \\'sun origin\\', and it is often called the \"Land of the Rising Sun\". Japan is a stratovolcanic archipelago consisting of about 6,852 islands. The four largest are Honshu, Hokkaido, Kyushu, and Shikoku, which make up about ninety-seven percent of Japan\\'s land area and often are referred to as home islands. The country is divided into 47 prefectures in eight regions, with Hokkaido being the northernmost prefecture and Okinawa being the southernmost one. Japan is the 2nd most populous island country. The population of 127 million is the world\\'s eleventh largest, of which 98.5% are ethnic Japanese. 90.7% of people live in cities, while 9.3% live in the countryside.[16] About 13.8 million people live in Tokyo,[17] the capital of Japan. The Greater Tokyo Area is the most populous metropolitan area in the world with over 38 million people.[18] Archaeological research indicates that Japan was inhabited as early as the Upper Paleolithic period. The first written mention of Japan is in Chinese history texts from the 1st century AD. Influence from other regions, mainly China, followed by periods of isolation, particularly from Western Europe, has characterized Japan\\'s history. From the 12th century until 1868, Japan was ruled by successive feudal military shÅguns who ruled in the name of the Emperor. Japan entered into a long period of isolation in the early 17th century, which was ended in 1853 when a United States fleet pressured Japan to open to the West. After nearly two decades of internal conflict and insurrection, the Imperial Court regained its political power in 1868 through the help of several clans from ChÅshÅ« and Satsuma â€“ and the Empire of Japan was established. In the late 19th and early 20th centuries, victories in the First Sino-Japanese War, the Russo-Japanese War and World War I allowed Japan to expand its empire during a period of increasing militarism. The Second Sino-Japanese War of 1937 expanded into part of World War II in 1941, which came to an end in 1945 following the Japanese surrender. Since adopting its revised constitution on May 3, 1947, during the occupation led by SCAP, the sovereign state of Japan has maintained a unitary parliamentary constitutional monarchy with an Emperor and an elected legislature called the National Diet. Japan is a member of the ASEAN Plus mechanism, UN, the OECD, the G7, the G8, and the G20, and is considered a great power.[19][20][21] Its economy is the world\\'s third-largest by nominal GDP and the fourth-largest by purchasing power parity. It is also the world\\'s fourth-largest exporter and fourth-largest importer. Japan benefits from a highly skilled and educated workforce; it has among the world\\'s largest proportion of citizens holding a tertiary education degree.[22] Although it has officially renounced its right to declare war, Japan maintains a modern military with the world\\'s eighth-largest military budget,[23] used for self-defense and peacekeeping roles; it ranked as the world\\'s fourth most-powerful military in 2015.[24] Japan is a highly developed country with a very high standard of living and Human Development Index. Its population enjoys the highest life expectancy and third lowest infant mortality rate in the world, but is experiencing issues due to an aging population and low birthrate. Japan is renowned for its historical and extensive cinema, influential music industry, anime, video gaming, rich cuisine and its major contributions to science and modern technology.[25][26]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPuzoxCacBZJ",
        "outputId": "21eb0f11-5ce4-4202-9203-3e5117f45874"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpQzg57nSAT2"
      },
      "source": [
        "# å¾Œã§æ¶ˆã™\n",
        "def preprocessing_text(text):\n",
        "  def cleaning_text(text):\n",
        "    # @ã®å‰Šé™¤\n",
        "    pattern1 = '@|%'\n",
        "    text = re.sub(pattern1, '', text)    \n",
        "    pattern2 = '\\[[0-9 ]*\\]'\n",
        "    text = re.sub(pattern2, '', text)    \n",
        "    # <b>ã‚¿ã‚°ã®å‰Šé™¤\n",
        "    pattern3 = '\\([a-z ]*\\)'\n",
        "    text = re.sub(pattern3, '', text)    \n",
        "    pattern4 = '[0-9]'\n",
        "    text = re.sub(pattern4, '', text)\n",
        "    return text\n",
        "  \n",
        "  def tokenize_text(text):\n",
        "    text = re.sub('[.,]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "  def lemmatize_word(word):\n",
        "    # make words lower  example: Python =>python\n",
        "    word=word.lower()\n",
        "    \n",
        "    # lemmatize  example: cooked=>cook\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "      return lemma\n",
        "    \n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
        "  tokens = [word for word in tokens if word is not None]\n",
        "  return tokens\n",
        "  \n",
        "docs = df[\"Abstract\"].values\n",
        "pp_docs = [preprocessing_text(text) for text in docs]\n",
        "tfidf_vector, word2id = tfidf_vectorizer(pp_docs)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNzPIvaxPOv5"
      },
      "source": [
        "word2id.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p73OT5sPs9y",
        "outputId": "6811fb4e-7112-44d2-fe71-9331d01548f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def calc_cosine(vector, vector_list):\n",
        "  result = {}\n",
        "  for i, x in enumerate(vector_list):\n",
        "    result[i] = cosine_similarity(vector, vector_list[i])\n",
        "    \n",
        "  return result\n",
        "\n",
        "print(\"tfidf\")\n",
        "res = calc_cosine(tfidf_vector[0],tfidf_vector)\n",
        "res"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0,\n",
              " 1: 0.04945156965230687,\n",
              " 2: 0.03550026859810149,\n",
              " 3: 0.07494324927746153,\n",
              " 4: 0.02200165046387345,\n",
              " 5: 0.089213868005443,\n",
              " 6: 0.04329186935344452,\n",
              " 7: 0.04340970910393382,\n",
              " 8: 0.050616794433693456,\n",
              " 9: 0.05446867547327852,\n",
              " 10: 0.03479541972998953,\n",
              " 11: 0.03392463518350004,\n",
              " 12: 0.038469390607195876,\n",
              " 13: 0.05035814117836253,\n",
              " 14: 0.06794378321355649,\n",
              " 15: 0.029516361108928312}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMGEF5UJUZSz",
        "outputId": "0c135370-113a-4dfe-fec5-1e175efcbbdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sorted(res.items(), key=lambda x:x[1],reverse=True)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1.0),\n",
              " (5, 0.089213868005443),\n",
              " (3, 0.07494324927746153),\n",
              " (14, 0.06794378321355649),\n",
              " (9, 0.05446867547327852),\n",
              " (8, 0.050616794433693456),\n",
              " (13, 0.05035814117836253),\n",
              " (1, 0.04945156965230687),\n",
              " (7, 0.04340970910393382),\n",
              " (6, 0.04329186935344452),\n",
              " (12, 0.038469390607195876),\n",
              " (2, 0.03550026859810149),\n",
              " (10, 0.03479541972998953),\n",
              " (11, 0.03392463518350004),\n",
              " (15, 0.029516361108928312),\n",
              " (4, 0.02200165046387345)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OWm8pnAMQH-"
      },
      "source": [
        "## Option 3\n",
        "\n",
        "### Word2Vec & Doc2Vec\n",
        "\n",
        "Word2Vecã‚„Doc2Vecã§ã¯å˜èªã®æ„å‘³ã‚’æ‰ãˆã‚‰ã‚Œã¦ã„ã‚‹ã‹ã®ã‚ˆã†ãªæ¼”ç®—ãŒå‡ºæ¥ã‚‹  \n",
        "King - Man + Woman = Queen ãªã©  \n",
        "è©³ç´°ã¯è¬›ç¾©ã‚¹ãƒ©ã‚¤ãƒ‰ã¸   \n",
        "\n",
        "å­¦ç¿’æ¸ˆã¿ã®word2vecãŒgithub( https://github.com/Kyubyong/wordvectors )ã«ä¸ŠãŒã£ã¦ã„ã‚‹ã®ã§  \n",
        "æ—¥æœ¬ã¨å„å›½ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†  \n",
        "è¶³ã—ç®—ã‚„å¼•ãç®—ãŒå‡ºæ¥ã‚‹ã®ã§ãã‚Œã‚‚è©¦ã—ã¦ã¿ã‚ˆã†  \n",
        "\n",
        "å‚è€ƒ : \"BOKU\"ã®ITãªæ—¥å¸¸ (https://arakan-pgm-ai.hatenablog.com/entry/2019/02/08/090000)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWddZ1mZXFsv"
      },
      "source": [
        ""
      ],
      "execution_count": 67,
      "outputs": []
    }
  ]
}