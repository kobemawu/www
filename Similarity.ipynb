{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similarity.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kobemawu/www/blob/master/Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aVVn6hB9Zei",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing and calculate similarity\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆã®ç›®æ¨™ã¯è‡ªåŠ›ã§æ–‡æ›¸ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨  \n",
        "æœ€çµ‚çš„ã«Wikipediaã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å›½ã®é¡ä¼¼åº¦ã‚’æ¸¬ã‚Š  \n",
        "æ—¥æœ¬ã¨ä¼¼ã¦ã„ã‚‹å›½ã‚’æ¢ã™"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5-315WVmN64",
        "colab_type": "code",
        "outputId": "8087c6f8-50b8-4a73-d384-56df8f3406ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.167)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.167 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.167)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8iK7xBDTmtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBUvXHj7mRls",
        "colab_type": "code",
        "outputId": "4a777a79-e94e-483b-dbfe-46747e214ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpg2beFhKA2R",
        "colab_type": "text"
      },
      "source": [
        "## 1. Calculate similarity\n",
        "\n",
        "ä»¥ä¸‹ã®ä¸‰ã¤ã®æ–‡ã‚’è€ƒãˆã‚‹  \n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "\n",
        "Doc Aã¨Doc Bã¯ä¼¼ã¦ã„ãã†ã ãŒã€Doc Cã¯Doc Aã¨ã‚‚Doc Bã¨ã‚‚ä¼¼ã¦ã„ãªã•ãã†  \n",
        "ã“ã‚Œã‚’é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ç¢ºã‹ã‚ã‚‹\n",
        "\n",
        "é¡ä¼¼åº¦ã®è¨ˆç®—ã®ä»•æ–¹ã¯ã„ãã¤ã‹ã‚ã‚‹\n",
        "\n",
        "- é›†åˆãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦\n",
        "  - Jaccardä¿‚æ•°\n",
        "  - Diceä¿‚æ•°\n",
        "  - Simpsonä¿‚æ•°\n",
        "- ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦\n",
        "  - ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢\n",
        "  - ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rpPCaUeOWE",
        "colab_type": "text"
      },
      "source": [
        "### é›†åˆãƒ™ãƒ¼ã‚¹\n",
        "\n",
        "æ–‡æ›¸ã‚’å˜èªã®é›†åˆã«å¤‰æ›ã™ã‚‹  \n",
        "é›†åˆãªã®ã§é‡è¤‡ã—ãŸå˜èªã¯å‰Šé™¤ã™ã‚‹  \n",
        "å‰å‡¦ç†ã¯ä»Šå›ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹   \n",
        "\n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "â†“    \n",
        "Set A : {'a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'}  \n",
        "Set B : {'an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'}  \n",
        "Set C : {'basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'}  \n",
        "\n",
        "ã“ã®é›†åˆãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’è¡¨ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uts5Ns2eKDaW",
        "colab_type": "text"
      },
      "source": [
        "#### Jaccardä¿‚æ•°\n",
        "Jaccardä¿‚æ•°ã¯äºŒã¤ã®é›†åˆA,Bã«å¯¾ã—ã¦å®šç¾©ã•ã‚Œã‚‹é¡ä¼¼åº¦ã§ã‚ã‚‹  \n",
        "è¨ˆç®—å¼ã¯ä»¥ä¸‹ã®é€šã‚Š\n",
        "\n",
        "\\begin{equation}\n",
        "J(A,B)=\\dfrac{|A\\cap B|}{|A \\cup B|}\n",
        "\\end{equation}\n",
        "\n",
        "å…±é€šéƒ¨åˆ†ã®å‰²åˆãŒå¤§ãã‘ã‚Œã°ãã®äºŒã¤ã®æ–‡æ›¸ã¯ä¼¼ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqr10Mw-K5UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard_similarity(set_a,set_b):\n",
        "  # ç©é›†åˆã®è¦ç´ æ•°ã‚’è¨ˆç®—\n",
        "  num_intersection = len(set.intersection(set_a, set_b))\n",
        "  # å’Œé›†åˆã®è¦ç´ æ•°ã‚’è¨ˆç®—\n",
        "  num_union = len(set.union(set_a, set_b))\n",
        "  #Jaccardä¿‚æ•°ã‚’ç®—å‡ºã€€ç©ºé›†åˆã®æ™‚ã¯1ã‚’å‡ºåŠ›\n",
        "  try:\n",
        "      return float(num_intersection) / num_union\n",
        "  except ZeroDivisionError:\n",
        "      return 1.0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZSFY5urK8eT",
        "colab_type": "code",
        "outputId": "e74717ef-99ae-4b97-85dc-ac99ed2fd3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"jaccard(a, b) = \", jaccard_similarity(set_a, set_b)) #Jaccardä¿‚æ•°ã‚’è¨ˆç®—\n",
        "print(\"jaccard(a, c) = \", jaccard_similarity(set_a, set_c))\n",
        "print(\"jaccard(b, c) = \", jaccard_similarity(set_b, set_c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jaccard(a, b) =  0.5714285714285714\n",
            "jaccard(a, c) =  0.11764705882352941\n",
            "jaccard(b, c) =  0.05555555555555555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXBk5SiTMmGf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "nltkã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹  \n",
        "å®šç¾©ã¨åŒã˜ã‚ˆã†ã«è¨ˆç®—ã‚’è¡Œã†ã®ã§ã€å…¥åŠ›ã¯é›†åˆ  \n",
        "è·é›¢ã«ãªã£ã¦ã„ã‚‹ã¨ã“ã‚ã«ã¯æ³¨æ„ãŒå¿…è¦"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZQ9q8mFLJTO",
        "colab_type": "code",
        "outputId": "ce84c908-60fa-4740-8d28-69305bd050a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from nltk.metrics import jaccard_distance\n",
        "\n",
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "# Jaccardè·é›¢ã«ãªã£ã¦ã„ã‚‹ã®ã§ã€é¡ä¼¼åº¦ã«å¤‰æ›ã™ã‚‹ã¨ãã¯1ã‹ã‚‰å¼•ã\n",
        "print(\"jaccard(a, b) = \", 1 - jaccard_distance(set_a, set_b))\n",
        "print(\"jaccard(a, c) = \", 1 - jaccard_distance(set_a, set_c))\n",
        "print(\"jaccard(b, c) = \", 1 - jaccard_distance(set_b, set_c))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jaccard(a,b) =  0.5714285714285714\n",
            "jaccard(a,c) =  0.11764705882352944\n",
            "jaccard(b,c) =  0.05555555555555558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lyCVTfePwB2",
        "colab_type": "text"
      },
      "source": [
        "#### SÃ¸rensen-Diceä¿‚æ•°\n",
        "\n",
        "Jaccardä¿‚æ•°ã§ã¯åˆ†æ¯ã¯ã®å’Œé›†åˆã§ã‚ã£ãŸãŸã‚  \n",
        "ç‰‡æ–¹ã®é›†åˆãŒã¨ã¦ã‚‚å¤§ãã„ã¨å…±é€šéƒ¨åˆ†ãŒå¤§ããã¦ã‚‚ä¿‚æ•°ã®å€¤ãŒå°ã•ããªã£ã¦ã—ã¾ã†ã¨ã„ã†å•é¡ŒãŒã‚ã‚‹  \n",
        "SÃ¸rensen-Diceä¿‚æ•°ã§ã¯ã€åˆ†æ¯ã‚’äºŒã¤ã®é›†åˆã®å¤§ãã•ã®å¹³å‡ã‚’ã¨ã‚‹ã“ã¨ã§ã€ãã®å½±éŸ¿ã‚’ç·©å’Œã—ã¦ã„ã‚‹  \n",
        "\n",
        "$\n",
        "DSC(A,B) = \\dfrac{|A\\cap B|}{\\dfrac{|A| + |B|}{2}} = \\dfrac{2|A\\cap B|}{|A| + |B|}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M0RikFPR3Qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice_similarity(set_a, set_b):\n",
        "  num_intersection =  len(set.intersection(set_a, set_b))\n",
        "  sum_nums = len(set_a) + len(set_b)\n",
        "  try:\n",
        "    return 2 * num_intersection / sum_nums\n",
        "  except ZeroDivisionError:\n",
        "    return 1.0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZQFbXlESPWl",
        "colab_type": "code",
        "outputId": "e5248c67-0b7e-4e0a-e541-517f22d6b1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"dice(a, b) = \", dice_similarity(set_a, set_b))\n",
        "print(\"dice(a, c) = \", dice_similarity(set_a, set_c))\n",
        "print(\"dice(b, c) = \", dice_similarity(set_b, set_c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dice(a, b) =  0.7272727272727273\n",
            "dice(a, c) =  0.21052631578947367\n",
            "dice(b, c) =  0.10526315789473684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxtliI-HPwRE",
        "colab_type": "text"
      },
      "source": [
        "#### Szymkiewicz-Simpsonä¿‚æ•°\n",
        "\n",
        "å·®é›†åˆã®è¦ç´ æ•°ã®å½±éŸ¿ã‚’æ¥µé™ã¾ã§æŠ‘ãˆãŸã®ãŒSzymkiewicz-Simpsonä¿‚æ•°    \n",
        "$\n",
        "overlap(ğ´,ğµ) = \\dfrac{|A\\cap B|}{\\min(|A|, |B|)}\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgGJ5GhmUduH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simpson_similarity(list_a, list_b):\n",
        "  num_intersection = len(set.intersection(set(list_a), set(list_b)))\n",
        "  min_num = min(len(set(list_a)), len(set(list_b)))\n",
        "  try:\n",
        "    return num_intersection / min_num\n",
        "  except ZeroDivisionError:\n",
        "    if num_intersection == 0:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_1Gjy4IV9eo",
        "colab_type": "code",
        "outputId": "bd752621-a182-4b35-d7f9-842de47d1a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"simpson(a, b) = \", simpson_similarity(set_a, set_b)) \n",
        "print(\"simpson(a, c) = \", simpson_similarity(set_a, set_c)) \n",
        "print(\"simpson(b, c) = \", simpson_similarity(set_b, set_c)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simpson(a, b) =  0.7272727272727273\n",
            "simpson(a, c) =  0.25\n",
            "simpson(b, c) =  0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-T32gmnZIyt",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 1\n",
        "è‰²ã€…ãªé›†åˆã‚’ä½œã£ã¦é›†åˆãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã®æ¯”è¼ƒã‚’ã—ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJwJpZpdYtz9",
        "colab_type": "code",
        "outputId": "a644cc06-c7b7-4f2b-82dc-ecd067767cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "set_d = set() # å¤§ãã‚ã®é›†åˆã‚’ä½œã£ã¦è©¦ã—ã¦ã¿ã‚ˆã†\n",
        "\n",
        "print(\"jaccard similarity:\")\n",
        "print(jaccard_similarity(set_d, set_a))\n",
        "print(jaccard_similarity(set_d, set_b))\n",
        "print(jaccard_similarity(set_d, set_c))\n",
        "\n",
        "print(\"dice similarity:\")\n",
        "print(dice_similarity(set_d, set_a))\n",
        "print(dice_similarity(set_d, set_b))\n",
        "print(dice_similarity(set_d, set_c))\n",
        "\n",
        "print(\"simpson similarity:\")\n",
        "print(simpson_similarity(set_d, set_a))\n",
        "print(simpson_similarity(set_d, set_b))\n",
        "print(simpson_similarity(set_d, set_c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jaccard similarity:\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "dice similarity:\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "simpson similarity:\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Hm34tgXCbh",
        "colab_type": "text"
      },
      "source": [
        "### ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ \n",
        "\n",
        "\n",
        "æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ç¾ã—é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹  \n",
        "ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®æ‰‹æ³•ã¯è‰²ã€…ã‚ã‚‹ãŒä»Šå›ã¯BoW(Bag of Words)ã§èª¬æ˜ã™ã‚‹  \n",
        "\n",
        "BoWã¯æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã™ã‚‹æ–¹æ³•ã®ä¸€ã¤  \n",
        "æƒ³å®šã—ã¦ã„ã‚‹å˜èªã®ç·æ•°ã‚’Nã¨ã™ã‚‹ã¨ã€å„æ¬¡å…ƒãŒå„å˜èªã«å¯¾å¿œã™ã‚‹Næ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è€ƒãˆã‚‹  \n",
        "å„æ¬¡å…ƒã®å€¤ã¯ãã®å˜èªãŒæ–‡æ›¸ä¸­ã§å‡ºãŸå›æ•°\n",
        "\n",
        "ä¾‹ï¼‰  \n",
        "Doc A : \"I like apples and a strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "â†“  \n",
        "å…¨å˜èªã¯19å€‹ã§ã€å„æ¬¡å…ƒã®å€¤ã¯ä»¥ä¸‹ã®å˜èªã®å€‹æ•°ã«å¯¾å¿œã™ã‚‹BoWã‚’è€ƒãˆã‚‹  \n",
        "['an', 'and', 'apple', 'apples', 'basketball', 'bought', 'buy', 'day', 'eat', 'every', 'i', 'jordan', 'like', 'michael', 'play', 'some', 'strawberries', 'tomorrow', 'will']  \n",
        "â†“  \n",
        "BoW A : [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]  \n",
        "BoW B : [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
        "BoW C : [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
        "\n",
        "ã“ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’è¡¨ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQOjHTfCeXjs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢\n",
        "\n",
        "å„æ–‡æ›¸ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ã™ã“ã¨ãŒå‡ºæ¥ãŸã®ã§  \n",
        "ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ãŒè¨ˆç®—ã§ãã‚‹  \n",
        "ã“ã®è·é›¢ãŒå°ã•ã‘ã‚Œã°ä¼¼ã¦ã„ã‚‹ã¨è€ƒãˆã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹\n",
        "\n",
        "\\begin{equation}\n",
        "d(v_1,v_2) =(\\sum_{i=1}^n (v_{1i}-v_{2i})^2)^{\\frac{1}{2}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtEzuSDKZbpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euclidean_distance(list_a, list_b):\n",
        "  diff_vec = np.array(list_a) - np.array(list_b)\n",
        "  return np.linalg.norm(diff_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sr_ZYj7azLV",
        "colab_type": "code",
        "outputId": "c10469e8-538e-4806-afce-01e994b769b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "bow_a = [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]  \n",
        "bow_b = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
        "bow_c = [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
        "\n",
        "print(\"euclidean_distance(bow_a, bow_b) = \",euclidean_distance(bow_a, bow_b))\n",
        "print(\"euclidean_distance(bow_a, bow_c) = \",euclidean_distance(bow_a, bow_c))\n",
        "print(\"euclidean_distance(bow_b, bow_c) = \",euclidean_distance(bow_b, bow_c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "euclidean_distance(bow_a, bow_b) =  2.23606797749979\n",
            "euclidean_distance(bow_a, bow_c) =  3.7416573867739413\n",
            "euclidean_distance(bow_b, bow_c) =  4.123105625617661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfKcQd5SiBb-",
        "colab_type": "text"
      },
      "source": [
        "#### ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼è·é›¢\n",
        "\n",
        "ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã‚’ä¸€èˆ¬åŒ–ã—ãŸè·é›¢\n",
        "pã®å€¤ã‚’å¤‰ãˆã‚‹ã“ã¨ã§è‰²ã€…ãªè·é›¢ã‚’è¡¨ç¾ã§ãã‚‹  \n",
        "\n",
        "\\begin{equation}\n",
        "d(v_1,v_2) = (\\sum_{i=1}^n |v_{1i}-v_{2i}|^p)^{\\frac{1}{p}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx46KLSNhjI2",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2\n",
        "ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼è·é›¢ã‚’è¨ˆç®—ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã„ã¦  \n",
        "p=1,2,3ã§è·é›¢ã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6UMFh58bexH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.linalg.normã«ã¤ã„ã¦èª¿ã¹ã‚ˆã†\n",
        "def minkowski_distance(list_a, list_b, p):\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKMIMbyJc8eH",
        "colab_type": "code",
        "outputId": "260027a1-408d-4b7a-9490-0e7c2314a0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# p=1\n",
        "print(minkowski_distance(bow_a, bow_b, 1))\n",
        "print(minkowski_distance(bow_a, bow_c, 1))\n",
        "print(minkowski_distance(bow_b, bow_c, 1))\n",
        "\n",
        "# p=2\n",
        "print(minkowski_distance(bow_a, bow_b, 2))\n",
        "print(minkowski_distance(bow_a, bow_c, 2))\n",
        "print(minkowski_distance(bow_b, bow_c, 2))\n",
        "\n",
        "# p=3\n",
        "print(minkowski_distance(bow_a, bow_b, 3))\n",
        "print(minkowski_distance(bow_a, bow_c, 3))\n",
        "print(minkowski_distance(bow_b, bow_c, 3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0\n",
            "14.0\n",
            "17.0\n",
            "5.0\n",
            "14.0\n",
            "17.0\n",
            "2.23606797749979\n",
            "3.7416573867739413\n",
            "4.123105625617661\n",
            "1.7099759466766968\n",
            "2.4101422641752297\n",
            "2.571281590658235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIyabGRTKGwA",
        "colab_type": "text"
      },
      "source": [
        "#### ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
        "\n",
        "ãƒ™ã‚¯ãƒˆãƒ«ã®ãªã™è§’ã«ç€ç›®ã—ã¦é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹  \n",
        "\n",
        "\\begin{equation}\n",
        "similarity(A, B)=cos(\\theta)=\\dfrac{\\sum_{i=1}^n A_iB_i}{{\\sqrt A}{\\sqrt B}}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWy619amlwp",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 3\n",
        "ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã„ã¦è¨ˆç®—ã—ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDhwqTN5yJGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy.array ã«ã¤ã„ã¦èª¿ã¹ã‚ˆã†\n",
        "def cosine_similarity(list_a, list_b):\n",
        "  # ã‚ã¨ã§æ¶ˆã™\n",
        "  inner_prod = np.array(list_a).dot(np.array(list_b))\n",
        "  norm_a = np.linalg.norm(list_a)\n",
        "  norm_b = np.linalg.norm(list_b)\n",
        "  try:\n",
        "      return inner_prod / (norm_a*norm_b)\n",
        "  except ZeroDivisionError:\n",
        "      return 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sew3u-YezRrX",
        "colab_type": "code",
        "outputId": "52aacae6-4480-4975-c8fe-415dccfa38af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "bow_a = [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]\n",
        "bow_b = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]\n",
        "bow_c = [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "print(\"cosine_similarity(bow_a, bow_b) = \",cosine_similarity(bow_a, bow_b))\n",
        "print(\"cosine_similarity(bow_a, bow_c) = \",cosine_similarity(bow_a, bow_c))\n",
        "print(\"cosine_similarity(bow_b, bow_c) = \",cosine_similarity(bow_b, bow_c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine_similarity(bow_a, bow_b) =  0.8153742483272114\n",
            "cosine_similarity(bow_a, bow_c) =  0.41812100500354543\n",
            "cosine_similarity(bow_b, bow_c) =  0.3223291856101521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao0jywXqKIFS",
        "colab_type": "text"
      },
      "source": [
        "### é›†åˆãƒ™ãƒ¼ã‚¹ã¨ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®æ¯”è¼ƒ\n",
        "\n",
        "é›†åˆæ¼”ç®—ã®æ–¹ã¯ä¸€ã¤ä¸€ã¤ã®æ–‡æ›¸ãŒå°ã•ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ€§èƒ½ãŒé«˜ã„  \n",
        "æ–‡æ›¸ãŒã‚ã‚‹ç¨‹åº¦å¤§ãããªã‚‹ã¨ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®æ–¹ãŒæœ‰ç”¨ã«ãªã‚‹  \n",
        "ãã®ä»£ã‚ã‚Šã€èªå½™é›†åˆãŒå¤§ãããªã‚Šè¨ˆç®—é‡ãŒå¤§ãããªã£ã¦ã—ã¾ã†\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1IXMwIyhU-6",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4\n",
        "çŸ­ã„æ–‡ç« ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨é•·ã„æ–‡ç« ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªåˆ†ã§ä½œã‚Š    \n",
        "Jaccardä¿‚æ•°ã¨ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦æ¯”è¼ƒã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpkcXVDBhZg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "short_docs = []\n",
        "long_docs = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuXSHk01KLza",
        "colab_type": "text"
      },
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "é›†åˆé–“ã®å…±é€šéƒ¨åˆ†ã‚„ãƒ™ã‚¯ãƒˆãƒ«é–“ã®è·é›¢ã‚„è§’åº¦ã§é¡ä¼¼åº¦ã‚’æ¸¬ã‚‹ã“ã¨ãŒå‡ºæ¥ãŸ  \n",
        "é›†åˆã‚„ãƒ™ã‚¯ãƒˆãƒ«ãŒæ–‡æ›¸ã®ç‰¹å¾´ã‚’ä¸Šæ‰‹ãè¡¨ã›ã¦ã„ãªã„ã¨é¡ä¼¼åº¦ãŒä¸Šæ‰‹ãæ¸¬ã‚Œãªã„  \n",
        "æ–‡æ›¸ã‹ã‚‰ã©ã®ã‚ˆã†ã«é›†åˆã‚„ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œã‚‹ã‹ãŒã¨ã¦ã‚‚å¤§äº‹  \n",
        " \n",
        "é©åˆ‡ãªå‰å‡¦ç†ã‚’è¡Œã†ã“ã¨ã§ç‰¹å¾´ã‚’æ‰ãˆãŸé¡ä¼¼åº¦ã‚’æ¸¬ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹    \n",
        "å¾ŒåŠã¯ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«çµã£ã¦ç·´ç¿’ã—ã¦ã„ã  \n",
        "\n",
        "1. Clearning\n",
        "2. Tokenize\n",
        "3. Stemming\n",
        "4. Remove stop words\n",
        "5. Vectorize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4ispTSLKM-z",
        "colab_type": "text"
      },
      "source": [
        "### 2-1. Clearning\n",
        "\n",
        "ä¸Šã®ä¾‹ã§ã¯ç¶ºéº—ãªæ–‡ç« ã°ã‹ã‚Šæ‰±ã£ã¦ã„ãŸãŒã€å®Ÿéš›ã¯ã‚‚ã£ã¨æ±šã„   \n",
        "Webã‹ã‚‰å–ã£ã¦ããŸãƒ‡ãƒ¼ã‚¿ã ã¨htmlã‚¿ã‚°ãŒæ®‹ã£ã¦ã„ãŸã‚Šã€å¤‰ãªè¨˜å·ãŒå…¥ã£ã¦ã„ãŸã‚Šã™ã‚‹  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM-e5AZT316r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents=[\"I like apples and a strawberries. I will buy an apple tomorrow @Fresco.\",\n",
        "           \"I bought some apples and strawberries. I will eat an apple <b>tomorrow.</b>\",\n",
        "           \"I play basketball every day. I like Michael Jordan (born February 17, 1963).\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh56eU_levzv",
        "colab_type": "text"
      },
      "source": [
        "ä»Šã¯ä¸‰ã¤ãªã®ã§æ‰‹å‹•ã§æ¶ˆã›ã‚‹ãŒ  \n",
        "å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ã¨ãã«ã¯è‡ªå‹•ã§ç¶ºéº—ã«ã§ããªã„ã¨ã„ã‘ãªã„  \n",
        "ç¶ºéº—ã«ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œã‚‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06coot4PnVgS",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 5\n",
        "\n",
        "æ­£è¦è¡¨ç¾ã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¶ºéº—ã«ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã“ã†\n",
        "\n",
        "å‚è€ƒ: æ­£è¦è¡¨ç¾ (https://uxmilk.jp/41416)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt3NL8Ux5Q4E",
        "colab_type": "code",
        "outputId": "84647b30-72a8-4a00-ad3a-cc8526d643ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import re\n",
        "\n",
        "def cleaning_text(text):\n",
        "    # @ã®å‰Šé™¤\n",
        "    pattern1 = '@'\n",
        "    text = re.sub(pattern1, '', text)    \n",
        "    # <b>ã‚¿ã‚°ã®å‰Šé™¤\n",
        "    pattern2 = # \n",
        "    text = re.sub(pattern2, '', text)    \n",
        "    # ()å†…ã‚’å‰Šé™¤\n",
        "    pattern3 = #\n",
        "    text = re.sub(pattern3, '', text)\n",
        "    return text\n",
        "  \n",
        "\n",
        "for text in documents:\n",
        "    print(cleaning_text(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like apples and a strawberries. I will buy an apple tomorrow Fresco.\n",
            "I bought some apples and strawberries. I will eat an apple tomorrow.\n",
            "I play basketball every day. I like Michael Jordan .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcgcKWkRflR5",
        "colab_type": "text"
      },
      "source": [
        "#### Option 1\n",
        "\n",
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¶ºéº—ã«ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã‚ˆã†\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsdHgV-h26y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = '<p><b>Natural language processing</b> (<b>NLP</b>) is a subfield of <a href=\"/wiki/Computer_science\" title=\"Computer science\">computer science</a>, <a href=\"/wiki/Information_engineering_(field)\" title=\"Information engineering (field)\">information engineering</a>, and <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of <a href=\"/wiki/Natural_language\" title=\"Natural language\">natural language</a> data.</p>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1doxCMAGr0_O",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edJ80nTbQgLi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 2-2. Tokenize\n",
        "\n",
        "ã¾ã æ–‡å­—åˆ—ã®ã¾ã¾ãªã®ã§ã€å˜èªã”ã¨ã«åŒºåˆ‡ã‚‹  \n",
        "è‹±èªã ã¨ç©ºç™½åŒºåˆ‡ã‚Šã§ã‚ˆã„ãŒæ—¥æœ¬èªã ã¨å°‘ã—é¢å€’  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGbqTvkyYl2S",
        "colab_type": "code",
        "outputId": "9c579d6e-133a-42da-8c4e-3322bc5be131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def tokenize_text(text):\n",
        "  text = re.sub('[.,]', '', text)\n",
        "  return text.split()\n",
        "\n",
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  print(tokenize_text(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'like', 'apples', 'and', 'a', 'strawberries', 'I', 'will', 'buy', 'an', 'apple', 'tomorrow', 'Fresco']\n",
            "['I', 'bought', 'some', 'apples', 'and', 'strawberries', 'I', 'will', 'eat', 'an', 'apple', 'tomorrow']\n",
            "['I', 'play', 'basketball', 'every', 'day', 'I', 'like', 'Michael', 'Jordan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K97zCvZBQik9",
        "colab_type": "text"
      },
      "source": [
        "### 2-3. Stemming, Lemmatize\n",
        "\n",
        "åŒã˜æ„å‘³ã®å˜èªã§ã‚‚ç•°ãªã‚‹å½¢ã‚’ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹  \n",
        "ãã‚Œã‚‰ã‚’åˆ¥ã®å˜èªã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã®ã¯ä¸è‡ªç„¶  \n",
        "å°æ–‡å­—ã«å¤‰æ›ã—ãŸå¾Œ  \n",
        "Stemmingã‚„Lemmatizeã¨ã„ã†å‡¦ç†ã§åŒã˜å½¢ã«ã™ã‚‹  \n",
        "ä»Šå›ã¯Lemmatizeã®ã¿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7R55rKxZtAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import wordnet as wn #lemmatizeé–¢æ•°ã®ãŸã‚ã®import\n",
        "\n",
        "def lemmatize_word(word):\n",
        "    # make words lower  example: Python =>python\n",
        "    word=word.lower()\n",
        "    \n",
        "    # lemmatize  example: cooked=>cook\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "      return lemma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0sEVvmwaCqA",
        "colab_type": "code",
        "outputId": "6fb86a52-14cf-47c8-b671-61bd174dcadb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  print([lemmatize_word(word) for word in tokens])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'like', 'apple', 'and', 'a', 'strawberry', 'i', 'will', 'buy', 'an', 'apple', 'tomorrow', 'fresco']\n",
            "['i', 'buy', 'some', 'apple', 'and', 'strawberry', 'i', 'will', 'eat', 'an', 'apple', 'tomorrow']\n",
            "['i', 'play', 'basketball', 'every', 'day', 'i', 'like', 'michael', 'jordan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOPIFqvjfxEg",
        "colab_type": "text"
      },
      "source": [
        "strawberriesâ†’strawberryã®ã‚ˆã†ã«èªã‚’æ¨™æº–å½¢ã«å¤‰æ›å‡ºæ¥ãŸ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpUxnB9kQlKv",
        "colab_type": "text"
      },
      "source": [
        "### 2-4. Remove stop words\n",
        "\n",
        "a, theãªã©ã®æ–‡ç« ã«å¯„ã‚‰ãšä¸€èˆ¬çš„ã«ä½¿ã‚ã‚Œã‚‹å† è©ã€ä»£åè©ã€å‰ç½®è©ãªã©ã‚’ä½¿ã£ã¦ã‚‚æ„å‘³ãŒãªã„  \n",
        "ãã‚Œã‚‰ã®å˜èªã¯stop wordã¨å‘¼ã°ã‚Œã‚‹  \n",
        "nltkã«ã¯å°‚é–€å®¶ãŒå®šç¾©ã—ãŸstop wordã®ãƒªã‚¹ãƒˆãŒã‚ã‚‹ã®ã§ãã‚Œã‚’ä½¿ã†  \n",
        "å¿…è¦ã«å¿œã˜ã¦stop wordã¯è‡ªåˆ†ã§ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ã¹ã  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbSLDay-a36N",
        "colab_type": "code",
        "outputId": "a53a1db9-6fa1-48da-d774-bc900fa0ef31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#1 nltkã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ\n",
        "en_stop = nltk.corpus.stopwords.words('english')\n",
        "print(en_stop)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX8eIyxfbo6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stopwords(word, stopwordset):\n",
        "  if word in stopwordset:\n",
        "    return None\n",
        "  else:\n",
        "    return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WckIX0ThbQMy",
        "colab_type": "code",
        "outputId": "40e02a66-8f45-4963-b41f-a4dd504052e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  print([remove_stopwords(word, en_stop) for word in tokens])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 'like', 'apple', None, None, 'strawberry', None, None, 'buy', None, 'apple', 'tomorrow', 'fresco']\n",
            "[None, 'buy', None, 'apple', None, 'strawberry', None, None, 'eat', None, 'apple', 'tomorrow']\n",
            "[None, 'play', 'basketball', 'every', 'day', None, 'like', 'michael', 'jordan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4kNHhr5f8Vt",
        "colab_type": "text"
      },
      "source": [
        "ä»Šå›ã¯ã“ã‚Œã ã‘ã§çµ‚ã‚ã‚Šã«ã™ã‚‹ãŒå˜èªã®å‰Šé™¤ã¯ã‹ãªã‚Šé‡è¦  \n",
        "å‡ºç¾é »åº¦ãŒæ¥µç«¯ã«ä½ã„å˜èªã‚’å‰Šé™¤ã—ãŸã‚Šã€å‹•è©ã¨åè©ã«é™å®šã™ã‚‹ãªã©è‰²ã€…ã‚ã‚‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgjfOITFxwx_",
        "colab_type": "code",
        "outputId": "6a49e871-c379-4057-b622-0fc432641bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def preprocessing_text(text):\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
        "  tokens = [word for word in tokens if word is not None]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "preprocessed_docs = [preprocessing_text(text) for text in documents]\n",
        "preprocessed_docs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['like', 'apple', 'strawberry', 'buy', 'apple', 'tomorrow', 'fresco'],\n",
              " ['buy', 'apple', 'strawberry', 'eat', 'apple', 'tomorrow'],\n",
              " ['play', 'basketball', 'every', 'day', 'like', 'michael', 'jordan']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCd6YpYbKJSJ",
        "colab_type": "text"
      },
      "source": [
        "### 2-5. Vectorize\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmrBtgPxcwX0",
        "colab_type": "text"
      },
      "source": [
        "#### BoW(Bag of Words)\n",
        "\n",
        "\n",
        "ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã®å‡ºç¾å›æ•°ã®ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ã—ãŸã‚‚ã®  \n",
        "äººæ‰‹ã§å˜èªã‚’æ•°ãˆãŸã‚Šã™ã‚‹ã®ã¯ä¸å¯èƒ½ãªã®ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å‡¦ç†ã‚’å®Œçµã—ã¦ã—ã¾ãŠã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S69MsPjHcvut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bow_vectorizer(docs):\n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "        \n",
        "  result_list = []\n",
        "  for doc in docs:\n",
        "    doc_vec = [0] * len(word2id)\n",
        "    for w in doc:\n",
        "      doc_vec[word2id[w]] += 1\n",
        "    result_list.append(doc_vec)\n",
        "  return result_list, word2id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq3TpP7KO-XP",
        "colab_type": "code",
        "outputId": "fc36ccec-ad25-447f-bf3c-fa621390c61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bow_vec, word2id = bow_vectorizer(preprocessed_docs)\n",
        "print(bow_vec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udTTbKv8oFpG",
        "colab_type": "code",
        "outputId": "96df7e79-7318-42e0-e649-a085a7748b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "word2id.items()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('like', 0), ('apple', 1), ('strawberry', 2), ('buy', 3), ('tomorrow', 4), ('fresco', 5), ('eat', 6), ('play', 7), ('basketball', 8), ('every', 9), ('day', 10), ('michael', 11), ('jordan', 12)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e8h1SMTcyD5",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF(Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "BoWã§ã¯å„å˜èªã®é‡ã¿ãŒåŒã˜ã ã£ãŸãŒã€å˜èªã«ã‚ˆã£ã¦é‡è¦åº¦ã¯å¤‰ã‚ã‚‹  \n",
        "å˜èªã®é‡è¦åº¦ã‚’è€ƒæ…®ã—ãŸã®ãŒTF-IDF  \n",
        "\n",
        "TF(t, d) = ã‚ã‚‹å˜èª(t)ã®ã‚ã‚‹æ–‡æ›¸(d)ã«ãŠã‘ã‚‹å‡ºç¾é »åº¦  \n",
        "IDF(t) = ã‚ã‚‹å˜èª(t)ãŒå…¨æ–‡æ›¸é›†åˆ(D)ä¸­ã«ã©ã‚Œã ã‘ã®æ–‡æ›¸ã§å‡ºç¾ã—ãŸã‹ã®é€†æ•°  \n",
        "\n",
        "TF-IDF(t,d) = TF(t, d) * IDF(t)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiKn2p7Bc0bN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_vectorizer(docs):\n",
        "  def tf(word2id, doc):\n",
        "    term_counts = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      term_counts[word2id[term]] = doc.count(term)\n",
        "    tf_values = list(map(lambda x: x/sum(term_counts), term_counts))\n",
        "    return tf_values\n",
        "  \n",
        "  def idf(word2id, docs):\n",
        "    idf = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      idf[word2id[term]] = np.log(len(docs) / sum([bool(term in doc) for doc in docs]))\n",
        "    return idf\n",
        "  \n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "  \n",
        "  return [[_tf*_idf for _tf, _idf in zip(tf(word2id, doc), idf(word2id, docs))] for doc in docs], word2id\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz5YnQZclGfL",
        "colab_type": "code",
        "outputId": "1f8c31c9-4a36-4566-8e20-b76e60b0bcb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "tfidf_vector, word2id = tfidf_vectorizer(preprocessed_docs)\n",
        "print(tfidf_vector)\n",
        "print(word2id.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.05792358687259491, 0.11584717374518982, 0.05792358687259491, 0.05792358687259491, 0.05792358687259491, 0.15694461266687282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.13515503603605478, 0.06757751801802739, 0.06757751801802739, 0.06757751801802739, 0.0, 0.1831020481113516, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05792358687259491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15694461266687282, 0.15694461266687282, 0.15694461266687282, 0.15694461266687282, 0.15694461266687282, 0.15694461266687282]]\n",
            "dict_items([('like', 0), ('apple', 1), ('strawberry', 2), ('buy', 3), ('tomorrow', 4), ('fresco', 5), ('eat', 6), ('play', 7), ('basketball', 8), ('every', 9), ('day', 10), ('michael', 11), ('jordan', 12)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxCJEOjXYSIP",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 6\n",
        "BoWã¨TF-IDFã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ãã‚Œãã‚Œè¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD4nID08s-21",
        "colab_type": "code",
        "outputId": "463337f2-d590-483e-d1dd-ff9ae15fd73b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfidf\n",
            "0-1 similarity: 0.4719198681637555\n",
            "0-2 similarity: 0.03803869439363926\n",
            "1-2 similarity: 0.0\n",
            "bow\n",
            "0-1 similarity: 0.8249579113843053\n",
            "0-2 similarity: 0.1259881576697424\n",
            "1-2 similarity: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAUfDUOmJMsp",
        "colab_type": "text"
      },
      "source": [
        "### Option 2\n",
        "scikit-learn, nltk gensimãã‚Œãã‚Œã«TF-IDFã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ãŒã‚ã‚‹  \n",
        "ãã‚Œãã‚Œã§TF-IDFã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2tZLjHNxRDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scikit-learnã®tfidfã€€ã‚ã¨ã§æ¶ˆã™\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x7nd2-ekIs-",
        "colab_type": "code",
        "outputId": "34a34a2d-2042-4819-81eb-6b31f26858de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#nltk ã®tf-idfã€€ã‚ã¨ã§æ¶ˆã™\n",
        "collection = nltk.TextCollection(docs)\n",
        "terms = list(set(collection))\n",
        "nltk_vector = []\n",
        "for doc in docs:\n",
        "  tmp_vec = np.zeros(len(word2id))\n",
        "  for term in word2id.keys():\n",
        "    tmp_vec[word2id[term]] = collection.tf_idf(term, doc)\n",
        "  nltk_vector.append(list(tmp_vec))\n",
        "print(nltk_vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0, 0.033788759009013694, 0.033788759009013694, 0.033788759009013694, 0.0915510240556758, 0.033788759009013694, 0.033788759009013694, 0.0915510240556758, 0.033788759009013694, 0.033788759009013694, 0.033788759009013694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.033788759009013694, 0.033788759009013694, 0.0, 0.033788759009013694, 0.033788759009013694, 0.0, 0.033788759009013694, 0.033788759009013694, 0.033788759009013694, 0.0915510240556758, 0.0915510240556758, 0.0915510240556758, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.04505167867868493, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12206803207423442, 0.12206803207423442, 0.12206803207423442, 0.12206803207423442, 0.12206803207423442, 0.12206803207423442]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEyOY2eQkRMc",
        "colab_type": "code",
        "outputId": "0d93c47b-47ad-4e11-d3fd-62b19e24888e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#gensim tf-idf ã‚ã¨ã§æ¶ˆã™\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "\n",
        "dictionary = corpora.Dictionary(docs)\n",
        "print('===å˜èª->idã®å¤‰æ›è¾æ›¸===')\n",
        "print(dictionary.token2id)\n",
        "print(word2id)\n",
        "\n",
        "corpus = list(map(dictionary.doc2bow, docs))\n",
        "test_model = models.TfidfModel(corpus)\n",
        "corpus_tfidf = test_model[corpus]\n",
        "\n",
        "print('===çµæœè¡¨ç¤º===')\n",
        "gensim_vector = []\n",
        "for doc in corpus_tfidf:\n",
        "  tmp_vec = [0] * len(word2id)\n",
        "  for word in doc:\n",
        "    key = dictionary[word[0]]\n",
        "    tmp_vec[word2id[key]] = word[1]\n",
        "  gensim_vector.append(tmp_vec)\n",
        "\n",
        "print(gensim_vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===å˜èª->idã®å¤‰æ›è¾æ›¸===\n",
            "{'a': 0, 'an': 1, 'and': 2, 'apple': 3, 'apples': 4, 'buy': 5, 'i': 6, 'like': 7, 'strawberries.': 8, 'tomorrow.': 9, 'will': 10, 'bought': 11, 'eat': 12, 'some': 13, 'basketball': 14, 'day.': 15, 'every': 16, 'jordan.': 17, 'michael': 18, 'play': 19}\n",
            "{'i': 0, 'like': 1, 'apples': 2, 'and': 3, 'a': 4, 'strawberries.': 5, 'will': 6, 'buy': 7, 'an': 8, 'apple': 9, 'tomorrow.': 10, 'bought': 11, 'some': 12, 'eat': 13, 'play': 14, 'basketball': 15, 'every': 16, 'day.': 17, 'michael': 18, 'jordan.': 19}\n",
            "===çµæœè¡¨ç¤º===\n",
            "[[0, 0.20996682609546996, 0.20996682609546996, 0.20996682609546996, 0.5689074861149032, 0.20996682609546996, 0.20996682609546996, 0.5689074861149032, 0.20996682609546996, 0.20996682609546996, 0.20996682609546996, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.185617413417644, 0.185617413417644, 0, 0.185617413417644, 0.185617413417644, 0, 0.185617413417644, 0.185617413417644, 0.185617413417644, 0.5029324775265576, 0.5029324775265576, 0.5029324775265576, 0, 0, 0, 0, 0, 0], [0, 0.1489905855640844, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.40369167389095173, 0.40369167389095173, 0.40369167389095173, 0.40369167389095173, 0.40369167389095173, 0.40369167389095173]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMjo7F6ZKOW1",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 7\n",
        "\n",
        "æ§˜ã€…ãªå›½ã®Wikipediaã«ãŠã‘ã‚‹abstractã‚’å–ã‚Šå‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã—ãŸ  \n",
        "https://drive.google.com/open?id=1i7tekPQRKaAwg-ze3kv5IsufMW13LkLo  \n",
        "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ã†  \n",
        "\n",
        "Cosineé¡ä¼¼åº¦ã®è¨ˆç®—ã‚’è¡Œã„ã€Japanã«ä¼¼ã¦ã„ã‚‹å›½Top5ã‚’è¡¨ç¤ºã—ã¦ã¿ã‚ˆã†  \n",
        "å‰å‡¦ç†ã‚’è‡ªåˆ†ãªã‚Šã«å·¥å¤«ã™ã‚‹ã“ã¨  \n",
        "æ³¨ï¼‰é¡ä¼¼åº¦ã¯ã‚ã¾ã‚Šé«˜ããªã‚‰ãªãã¦ã‚‚è‰¯ã„  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1IbA30KKElT",
        "colab_type": "code",
        "outputId": "755886e1-6ce4-4547-fcd9-83f8cd760601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "df = pd.read_csv(\"./nlp_country.csv\")\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Japan</td>\n",
              "      <td>Japan is an island country in East Asia. Locat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>United States</td>\n",
              "      <td>The United States of America (USA), commonly k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>England</td>\n",
              "      <td>England is a country that is part of the Unite...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>China</td>\n",
              "      <td>China, officially the People's Republic of Chi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>India</td>\n",
              "      <td>India, also known as the Republic of India,[19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Korea</td>\n",
              "      <td>Korea is a region in East Asia.[3] Since 1948 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Germany</td>\n",
              "      <td>Germany, officially the Federal Republic of Ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Russia</td>\n",
              "      <td>Russia, or the Russian Federation[12], is a tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>France</td>\n",
              "      <td>France, officially the French Republic, is a c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Italy</td>\n",
              "      <td>Italy, officially the Italian Republic,[10][11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Brazil</td>\n",
              "      <td>Brazil officially the Federative Republic of B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Canada</td>\n",
              "      <td>Canada is a country in the northern part of No...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Spain</td>\n",
              "      <td>Spain, officially the Kingdom of Spain[11][a][...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Australia</td>\n",
              "      <td>Australia, officially the Commonwealth of Aust...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Indonesia</td>\n",
              "      <td>Indonesia, officially the Republic of Indonesi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Mexico</td>\n",
              "      <td>Mexico, officially the United Mexican States (...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Name                                           Abstract\n",
              "0           Japan  Japan is an island country in East Asia. Locat...\n",
              "1   United States  The United States of America (USA), commonly k...\n",
              "2         England  England is a country that is part of the Unite...\n",
              "3           China  China, officially the People's Republic of Chi...\n",
              "4           India  India, also known as the Republic of India,[19...\n",
              "5           Korea  Korea is a region in East Asia.[3] Since 1948 ...\n",
              "6         Germany  Germany, officially the Federal Republic of Ge...\n",
              "7          Russia  Russia, or the Russian Federation[12], is a tr...\n",
              "8          France  France, officially the French Republic, is a c...\n",
              "9           Italy  Italy, officially the Italian Republic,[10][11...\n",
              "10         Brazil  Brazil officially the Federative Republic of B...\n",
              "11         Canada  Canada is a country in the northern part of No...\n",
              "12          Spain  Spain, officially the Kingdom of Spain[11][a][...\n",
              "13      Australia  Australia, officially the Commonwealth of Aust...\n",
              "14      Indonesia  Indonesia, officially the Republic of Indonesi...\n",
              "15         Mexico  Mexico, officially the United Mexican States (..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2ZqrWJhBAO2",
        "colab_type": "code",
        "outputId": "411c6f83-dfa4-4751-b973-1cc1d27e2a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df.iloc[0][\"Abstract\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Japan is an island country in East Asia. Located in the Pacific Ocean, it lies off the eastern coast of the Asian continent and stretches from the Sea of Okhotsk in the north to the East China Sea and the Philippine Sea in the south. The kanji that make up Japan\\'s name mean \\'sun origin\\', and it is often called the \"Land of the Rising Sun\". Japan is a stratovolcanic archipelago consisting of about 6,852 islands. The four largest are Honshu, Hokkaido, Kyushu, and Shikoku, which make up about ninety-seven percent of Japan\\'s land area and often are referred to as home islands. The country is divided into 47 prefectures in eight regions, with Hokkaido being the northernmost prefecture and Okinawa being the southernmost one. Japan is the 2nd most populous island country. The population of 127 million is the world\\'s eleventh largest, of which 98.5% are ethnic Japanese. 90.7% of people live in cities, while 9.3% live in the countryside.[16] About 13.8 million people live in Tokyo,[17] the capital of Japan. The Greater Tokyo Area is the most populous metropolitan area in the world with over 38 million people.[18] Archaeological research indicates that Japan was inhabited as early as the Upper Paleolithic period. The first written mention of Japan is in Chinese history texts from the 1st century AD. Influence from other regions, mainly China, followed by periods of isolation, particularly from Western Europe, has characterized Japan\\'s history. From the 12th century until 1868, Japan was ruled by successive feudal military shÅguns who ruled in the name of the Emperor. Japan entered into a long period of isolation in the early 17th century, which was ended in 1853 when a United States fleet pressured Japan to open to the West. After nearly two decades of internal conflict and insurrection, the Imperial Court regained its political power in 1868 through the help of several clans from ChÅshÅ« and Satsuma â€“ and the Empire of Japan was established. In the late 19th and early 20th centuries, victories in the First Sino-Japanese War, the Russo-Japanese War and World War I allowed Japan to expand its empire during a period of increasing militarism. The Second Sino-Japanese War of 1937 expanded into part of World War II in 1941, which came to an end in 1945 following the Japanese surrender. Since adopting its revised constitution on May 3, 1947, during the occupation led by SCAP, the sovereign state of Japan has maintained a unitary parliamentary constitutional monarchy with an Emperor and an elected legislature called the National Diet. Japan is a member of the ASEAN Plus mechanism, UN, the OECD, the G7, the G8, and the G20, and is considered a great power.[19][20][21] Its economy is the world\\'s third-largest by nominal GDP and the fourth-largest by purchasing power parity. It is also the world\\'s fourth-largest exporter and fourth-largest importer. Japan benefits from a highly skilled and educated workforce; it has among the world\\'s largest proportion of citizens holding a tertiary education degree.[22] Although it has officially renounced its right to declare war, Japan maintains a modern military with the world\\'s eighth-largest military budget,[23] used for self-defense and peacekeeping roles; it ranked as the world\\'s fourth most-powerful military in 2015.[24] Japan is a highly developed country with a very high standard of living and Human Development Index. Its population enjoys the highest life expectancy and third lowest infant mortality rate in the world, but is experiencing issues due to an aging population and low birthrate. Japan is renowned for its historical and extensive cinema, influential music industry, anime, video gaming, rich cuisine and its major contributions to science and modern technology.[25][26]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpQzg57nSAT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# å¾Œã§æ¶ˆã™\n",
        "def preprocessing_text(text):\n",
        "  def cleaning_text(text):\n",
        "    # @ã®å‰Šé™¤\n",
        "    pattern1 = '@|%'\n",
        "    text = re.sub(pattern1, '', text)    \n",
        "    pattern2 = '\\[[0-9 ]*\\]'\n",
        "    text = re.sub(pattern2, '', text)    \n",
        "    # <b>ã‚¿ã‚°ã®å‰Šé™¤\n",
        "    pattern3 = '\\([a-z ]*\\)'\n",
        "    text = re.sub(pattern3, '', text)    \n",
        "    pattern4 = '[0-9]'\n",
        "    text = re.sub(pattern4, '', text)\n",
        "    return text\n",
        "  \n",
        "  def tokenize_text(text):\n",
        "    text = re.sub('[.,]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "  def lemmatize_word(word):\n",
        "    # make words lower  example: Python =>python\n",
        "    word=word.lower()\n",
        "    \n",
        "    # lemmatize  example: cooked=>cook\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "      return lemma\n",
        "    \n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
        "  tokens = [word for word in tokens if word is not None]\n",
        "  return tokens\n",
        "  \n",
        "docs = df[\"Abstract\"].values\n",
        "pp_docs = [preprocessing_text(text) for text in docs]\n",
        "tfidf_vector, word2id = tfidf_vectorizer(pp_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNzPIvaxPOv5",
        "colab_type": "code",
        "outputId": "843e5418-5a17-4a1e-86ea-fc8e5e7c7984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "word2id.items()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('alibaba', 0), ('group', 1), ('holding', 2), ('limited', 3), ('chinese', 4), ('multinational', 5), ('conglomerate', 6), ('company', 7), ('specialize', 8), ('e-commerce', 9), ('retail', 10), ('internet', 11), ('technology', 12), ('found', 13), ('april', 14), ('provide', 15), ('consumer-to-consumer', 16), ('(cc)', 17), ('business-to-consumer', 18), ('(bc)', 19), ('business-to-business', 20), ('(bb)', 21), ('sales', 22), ('services', 23), ('via', 24), ('web', 25), ('portal', 26), ('well', 27), ('electronic', 28), ('payment', 29), ('shopping', 30), ('search', 31), ('engine', 32), ('cloud', 33), ('computing', 34), ('operate', 35), ('diverse', 36), ('array', 37), ('business', 38), ('around', 39), ('world', 40), ('numerous', 41), ('sector', 42), ('name', 43), ('one', 44), (\"world's\", 45), ('admire', 46), ('fortune', 47), ('closing', 48), ('time', 49), ('date', 50), ('initial', 51), ('public', 52), ('offering', 53), ('(ipo)', 54), ('â€“', 55), ('us$', 56), ('billion', 57), ('high', 58), ('history', 59), ('september', 60), (\"alibaba's\", 61), ('market', 62), ('value', 63), ('wa', 64), ('december', 65), ('cap', 66), ('stand', 67), ('top', 68), ('valuable', 69), ('big', 70), ('january', 71), ('become', 72), ('second', 73), ('asian', 74), ('break', 75), ('valuation', 76), ('mark', 77), ('competitor', 78), ('tencent', 79), ('ha', 80), ('th', 81), ('global', 82), ('brand', 83), ('large', 84), ('retailer', 85), ('ai', 86), ('venture', 87), ('capital', 88), ('firm', 89), ('investment', 90), ('corporation', 91), ('host', 92), ('bb', 93), ('(alibabacom)', 94), ('cc', 95), ('(taobao)', 96), ('bc', 97), ('(tmall)', 98), ('marketplace', 99), ('online', 100), ('profits', 101), ('surpass', 102), ('us', 103), ('(including', 104), ('walmart', 105), ('amazon', 106), ('ebay)', 107), ('combine', 108), ('since', 109), ('expand', 110), ('medium', 111), ('industry', 112), ('revenue', 113), ('rising', 114), ('triple', 115), ('percentage', 116), ('point', 117), ('year', 118), ('also', 119), ('set', 120), ('record', 121), ('edition', 122), (\"china's\", 123), (\"singles'\", 124), ('day', 125), ('offline', 126), ('amazoncom', 127), ('inc', 128), ('american', 129), ('base', 130), ('seattle', 131), ('washington', 132), ('focus', 133), ('digital', 134), ('streaming', 135), ('artificial', 136), ('intelligence', 137), ('consider', 138), ('four', 139), ('along', 140), ('google', 141), ('apple', 142), ('facebook', 143), ('know', 144), ('disruption', 145), ('well-established', 146), ('technological', 147), ('innovation', 148), ('mass', 149), ('scale', 150), ('assistant', 151), ('provider', 152), ('platform', 153), ('measure', 154), ('capitalization', 155), ('employer', 156), ('unite', 157), ('state', 158), ('jeff', 159), ('bezos', 160), ('july', 161), ('bellevue', 162), ('initially', 163), ('start', 164), ('book', 165), ('later', 166), ('sell', 167), ('electronics', 168), ('software', 169), ('video', 170), ('game', 171), ('apparel', 172), ('furniture', 173), ('food', 174), ('toy', 175), ('jewelry', 176), ('acquire', 177), ('whole', 178), ('$', 179), ('vastly', 180), ('increase', 181), (\"amazon's\", 182), ('presence', 183), ('brick-and-mortar', 184), ('announce', 185), ('two-day', 186), ('delivery', 187), ('service', 188), ('prime', 189), ('million', 190), ('subscriber', 191), ('worldwide', 192), ('distribute', 193), ('download', 194), ('music', 195), ('audiobook', 196), ('audible', 197), ('subsidiary', 198), ('publishing', 199), ('arm', 200), ('film', 201), ('television', 202), ('studio', 203), ('produce', 204), ('consumer', 205), ('include', 206), ('kindle', 207), ('e-readers', 208), ('fire', 209), ('tablet', 210), ('tv', 211), ('echo', 212), ('devices', 213), ('addition', 214), ('ring', 215), ('twitchtv', 216), ('imdb', 217), ('among', 218), ('various', 219), ('controversy', 220), ('criticise', 221), ('poor', 222), ('working', 223), ('conditions', 224), ('tax', 225), ('avoidance', 226), ('anti-competitive', 227), ('practice', 228), ('headquarter', 229), ('cupertino', 230), ('california', 231), ('design', 232), ('develop', 233), ('computer', 234), (\"company's\", 235), ('hardware', 236), ('product', 237), ('iphone', 238), ('smartphone', 239), ('ipad', 240), ('mac', 241), ('personal', 242), ('ipod', 243), ('portable', 244), ('player', 245), ('watch', 246), ('smartwatch', 247), ('airpods', 248), ('wireless', 249), ('earbuds', 250), ('homepod', 251), ('smart', 252), ('speaker', 253), (\"apple's\", 254), ('macos', 255), ('io', 256), ('ipados', 257), ('watchos', 258), ('tvos', 259), ('system', 260), ('itunes', 261), ('safari', 262), ('browser', 263), ('ilife', 264), ('iwork', 265), ('creativity', 266), ('productivity', 267), ('suite', 268), ('professional', 269), ('application', 270), ('like', 271), ('final', 272), ('cut', 273), ('pro', 274), ('logic', 275), ('xcode', 276), ('store', 277), ('app', 278), ('tv+', 279), ('imessage', 280), ('icloud', 281), ('genius', 282), ('bar', 283), ('applecare', 284), ('pay', 285), ('cash', 286), ('card', 287), ('steve', 288), ('job', 289), ('wozniak', 290), ('ronald', 291), ('wayne', 292), (\"wozniak's\", 293), ('though', 294), ('share', 295), ('back', 296), ('within', 297), ('days', 298), ('incorporate', 299), ('ii', 300), ('grow', 301), ('quickly', 302), ('years', 303), ('hire', 304), ('staff', 305), ('designer', 306), ('production', 307), ('line', 308), ('go', 309), ('instant', 310), ('financial', 311), ('success', 312), ('next', 313), ('ship', 314), ('new', 315), ('feature', 316), ('innovative', 317), ('graphical', 318), ('user', 319), ('interface', 320), ('original', 321), ('macintosh', 322), ('marketing', 323), ('advertisement', 324), ('receive', 325), ('widespread', 326), ('critical', 327), ('acclaim', 328), ('however', 329), ('price', 330), ('library', 331), ('cause', 332), ('problem', 333), ('power', 334), ('struggle', 335), ('executive', 336), ('departed', 337), ('amicably', 338), ('remain', 339), ('honorary', 340), ('employee', 341), ('others', 342), ('resign', 343), ('evolve', 344), ('lost', 345), ('lower-priced', 346), ('duopoly', 347), ('microsoft', 348), ('windows', 349), ('intel', 350), ('pc', 351), ('clone', 352), ('board', 353), ('recruit', 354), ('ceo', 355), ('gil', 356), ('amelio', 357), ('would', 358), ('-day', 359), ('charge', 360), ('rehabilitate', 361), ('financially', 362), ('trouble', 363), ('companyâ€”reshaping', 364), ('layoff', 365), ('restructure', 366), ('led', 367), ('buy', 368), ('solving', 369), ('desperately', 370), ('fail', 371), ('strategy', 372), ('bringing', 373), ('pensively', 374), ('regain', 375), ('leadership', 376), ('status', 377), ('swiftly', 378), ('return', 379), ('profitability', 380), ('revitalize', 381), ('think', 382), ('different', 383), ('campaign', 384), ('rebuild', 385), ('launching', 386), ('imac', 387), ('opening', 388), ('chain', 389), ('acquiring', 390), ('broaden', 391), ('portfolio', 392), ('rename', 393), ('reflect', 394), ('shift', 395), ('toward', 396), ('launch', 397), ('great', 398), ('august', 399), ('due', 400), ('health', 401), ('complication', 402), ('tim', 403), ('cook', 404), ('two', 405), ('month', 406), ('die', 407), ('marking', 408), ('end', 409), ('era', 410), ('size', 411), ('annual', 412), ('total', 413), ('fiscal', 414), ('third-largest', 415), ('mobile', 416), ('phone', 417), ('manufacturer', 418), ('samsung', 419), ('huawei', 420), ('first', 421), ('trillion', 422), ('employ', 423), ('full-time', 424), ('maintain', 425), ('country', 426), ('actively', 427), ('use', 428), ('level', 429), ('loyalty', 430), ('rank', 431), ('significant', 432), ('criticism', 433), ('regard', 434), ('labor', 435), ('contractor', 436), ('environmental', 437), ('unethical', 438), ('behavior', 439), ('origin', 440), ('source', 441), ('material', 442), ('social', 443), ('network', 444), ('menlo', 445), ('park', 446), ('zuckerberg', 447), ('fellow', 448), ('harvard', 449), ('college', 450), ('student', 451), ('roommate', 452), ('eduardo', 453), ('saverin', 454), ('andrew', 455), ('mccollum', 456), ('dustin', 457), ('moskovitz', 458), ('chris', 459), ('hughes', 460), ('founder', 461), (\"website's\", 462), ('membership', 463), ('subsequently', 464), ('columbia', 465), ('stanford', 466), ('yale', 467), ('eventually', 468), ('ivy', 469), ('league', 470), ('school', 471), ('mit', 472), ('higher', 473), ('education', 474), ('institution', 475), ('boston', 476), ('area', 477), ('university', 478), ('lastly', 479), ('anyone', 480), ('claim', 481), ('least', 482), ('old', 483), ('allow', 484), ('register', 485), ('may', 486), ('vary', 487), ('depend', 488), ('local', 489), ('laws', 490), ('come', 491), ('face', 492), ('directory', 493), ('often', 494), ('given', 495), ('hold', 496), ('february', 497), ('newly', 498), ('list', 499), ('make', 500), ('appear', 501), ('onscreen', 502), (\"users'\", 503), ('news', 504), ('feed', 505), ('access', 506), ('connectivity', 507), ('smartphones', 508), ('create', 509), ('customize', 510), ('profile', 511), ('revealing', 512), ('information', 513), ('post', 514), ('text', 515), ('photo', 516), ('multimedia', 517), ('agree', 518), ('\"friend\"', 519), ('embed', 520), ('apps', 521), ('join', 522), ('common-interest', 523), ('notification', 524), (\"friends'\", 525), ('activity', 526), ('monthly', 527), ('active', 528), ('fake', 529), ('account', 530), ('catch', 531), ('miss', 532), ('real', 533), ('many', 534), ('critic', 535), ('question', 536), ('whether', 537), ('actual', 538), ('prominent', 539), ('coverage', 540), ('involve', 541), ('privacy', 542), ('(as', 543), ('cambridge', 544), ('analytica', 545), ('data', 546), ('scandal)', 547), ('political', 548), ('manipulation', 549), ('elections)', 550), ('psychological', 551), ('effects', 552), ('addiction', 553), ('low', 554), ('self-esteem', 555), ('content', 556), ('find', 557), ('objectionable', 558), ('conspiracy', 559), ('theory', 560), ('copyright', 561), ('infringement', 562), ('doe', 563), ('remove', 564), ('false', 565), ('page', 566), ('bring', 567), ('continuous', 568), ('commentator', 569), ('help', 570), ('spread', 571), ('offer', 572), ('instagram', 573), ('whatsapp', 574), ('oculus', 575), ('grokstyle', 576), ('independently', 577), ('messenger', 578), ('llc', 579), ('internet-related', 580), ('advertising', 581), ('alongside', 582), ('larry', 583), ('sergey', 584), ('brin', 585), ('phd', 586), ('together', 587), ('percent', 588), ('control', 589), ('stockholder', 590), ('voting', 591), ('supervoting', 592), ('stock', 593), ('privately', 594), ('take', 595), ('place', 596), ('move', 597), ('headquarters', 598), ('mountain', 599), ('view', 600), ('nickname', 601), ('googleplex', 602), ('plan', 603), ('reorganize', 604), ('interest', 605), ('call', 606), ('alphabet', 607), (\"alphabet's\", 608), ('leading', 609), ('continue', 610), ('umbrella', 611), ('sundar', 612), ('pichai', 613), ('appoint', 614), ('replacing', 615), ('rapid', 616), ('growth', 617), ('incorporation', 618), ('trigger', 619), ('acquisition', 620), ('partnership', 621), ('beyond', 622), (\"google's\", 623), ('core', 624), ('(google', 625), ('search)', 626), ('work', 627), ('doc', 628), ('sheet', 629), ('slides)', 630), ('email', 631), ('(gmail/inbox)', 632), ('scheduling', 633), ('management', 634), ('calendar)', 635), ('storage', 636), ('drive)', 637), ('messaging', 638), ('chat', 639), ('allo', 640), ('duo', 641), ('hangouts)', 642), ('language', 643), ('translation', 644), ('translate)', 645), ('mapping', 646), ('navigation', 647), ('map', 648), ('waze', 649), ('earth', 650), ('street', 651), ('view)', 652), ('sharing', 653), ('(youtube)', 654), ('note-taking', 655), ('keep)', 656), ('organize', 657), ('editing', 658), ('photos)', 659), ('lead', 660), ('development', 661), ('android', 662), ('chrome', 663), ('os', 664), ('lightweight', 665), ('increasingly', 666), ('hardware;', 667), ('partner', 668), ('major', 669), ('nexus', 670), ('release', 671), ('multiple', 672), ('october', 673), ('pixel', 674), ('home', 675), ('wifi', 676), ('mesh', 677), ('router', 678), ('daydream', 679), ('virtual', 680), ('reality', 681), ('headset', 682), ('experiment', 683), ('carrier', 684), ('fiber', 685), ('fi', 686), ('station)', 687), ('googlecom', 688), ('visit', 689), ('website', 690), ('several', 691), ('figure', 692), ('youtube', 693), ('blogger', 694), ('issue', 695), ('concern', 696), ('antitrust', 697), ('censorship', 698), ('neutrality', 699), ('mission', 700), ('statement', 701), ('\"to', 702), ('universally', 703), ('accessible', 704), ('useful\"', 705), ('unofficial', 706), ('slogan', 707), ('\"don\\'t', 708), ('evil\"', 709), ('code', 710), ('conduct', 711), ('reinstate', 712), ('international', 713), ('machine', 714), ('(ibm)', 715), ('armonk', 716), ('york', 717), ('operations', 718), ('begin', 719), ('endicott', 720), ('computing-tabulating-recording', 721), ('(ctr)', 722), ('\"international', 723), ('machines\"', 724), ('ibm', 725), ('middleware', 726), ('consult', 727), ('range', 728), ('mainframe', 729), ('nanotechnology', 730), ('research', 731), ('organization', 732), ('patent', 733), ('generate', 734), (')', 735), ('consecutive', 736), ('invention', 737), ('automate', 738), ('teller', 739), ('(atm)', 740), ('floppy', 741), ('disk', 742), ('hard', 743), ('drive', 744), ('magnetic', 745), ('stripe', 746), ('relational', 747), ('database', 748), ('sql', 749), ('programming', 750), ('upc', 751), ('barcode', 752), ('dynamic', 753), ('random-access', 754), ('memory', 755), ('(dram)', 756), ('exemplify', 757), ('system/', 758), ('dominant', 759), ('continually', 760), ('focusing', 761), ('higher-value', 762), ('profitable', 763), ('spinning', 764), ('printer', 765), ('lexmark', 766), ('sale', 767), ('(thinkpad/thinkcentre)', 768), ('x-based', 769), ('server', 770), ('lenovo', 771), ('(in', 772), ('respectively)', 773), ('pwc', 774), ('()', 775), ('spss', 776), ('weather', 777), ('red', 778), ('hat', 779), ('(agreement', 780), ('half', 781), ('\"fabless\"', 782), ('semiconductor', 783), ('offload', 784), ('manufacturing', 785), ('globalfoundries', 786), ('blue', 787), ('dow', 788), ('jones', 789), ('industrial', 790), ('average', 791), ('\"ibmers\"', 792), ('ibmers', 793), ('outside', 794), ('number', 795), ('india', 796), ('award', 797), ('five', 798), ('nobel', 799), ('prize', 800), ('six', 801), ('turing', 802), ('ten', 803), ('national', 804), ('medal', 805), ('(usa)', 806), ('science', 807), ('redmond', 808), ('manufacture', 809), ('license', 810), ('support', 811), ('relate', 812), ('best', 813), ('office', 814), ('explorer', 815), ('edge', 816), ('flagship', 817), ('xbox', 818), ('console', 819), ('surface', 820), ('lineup', 821), ('touchscreen', 822), ('maker', 823), ('word', 824), ('\"microsoft\"', 825), ('portmanteau', 826), ('\"microcomputer\"', 827), ('\"software\"', 828), ('ranking', 829), ('bill', 830), ('gates', 831), ('paul', 832), ('allen', 833), ('basic', 834), ('interpreter', 835), ('altair', 836), ('rose', 837), ('dominate', 838), ('ms-dos', 839), ('mid-s', 840), ('follow', 841), ('subsequent', 842), ('rise', 843), ('three', 844), ('billionaire', 845), ('estimate', 846), ('millionaire', 847), ('diversify', 848), ('corporate', 849), ('linkedin', 850), ('skype', 851), ('market-dominant', 852), ('compatible', 853), ('although', 854), ('majority', 855), ('overall', 856), ('wide', 857), ('enterprise', 858), ('desktop', 859), ('laptop', 860), ('tab', 861), ('gadget', 862), ('etc', 863), ('(with', 864), ('bing)', 865), ('(through', 866), ('msn)', 867), ('mix', 868), ('(hololens)', 869), ('(azure)', 870), ('(visual', 871), ('studio)', 872), ('ballmer', 873), ('replace', 874), ('envision', 875), ('\"devices', 876), ('services\"', 877), ('danger', 878), ('entering', 879), ('june', 880), ('computers;', 881), ('form', 882), (\"nokia's\", 883), ('division', 884), ('satya', 885), ('nadella', 886), ('instead', 887), ('reach', 888), ('publicly', 889), ('trade', 890), ('dethrone', 891), ('tech', 892), ('giant', 893), ('third', 894), ('respectively', 895), ('trillion-dollar', 896), ('netflix', 897), ('media-services', 898), ('los', 899), ('gatos', 900), ('reed', 901), ('hastings', 902), ('marc', 903), ('randolph', 904), ('scott', 905), ('valley', 906), ('primary', 907), ('subscription-based', 908), ('ott', 909), ('program', 910), ('in-house', 911), ('subscription', 912), ('free', 913), ('trial', 914), ('available', 915), ('almost', 916), ('except', 917), ('mainland', 918), ('china', 919), ('syria', 920), ('north', 921), ('korea', 922), ('crimea', 923), ('(due', 924), ('sanctions)', 925), ('netherlands', 926), ('brazil', 927), ('japan', 928), ('south', 929), ('member', 930), ('motion', 931), ('picture', 932), ('association', 933), ('america', 934), ('(mpaa)', 935), (\"netflix's\", 936), ('model', 937), ('dvd', 938), ('rental', 939), ('mail', 940), ('abandon', 941), ('founding', 942), ('introduction', 943), ('retain', 944), ('blu-ray', 945), ('internationally', 946), ('canada', 947), ('latin', 948), ('caribbean', 949), ('enter', 950), ('content-production', 951), ('debut', 952), ('series', 953), ('lilyhammer', 954), ('role', 955), ('producer', 956), ('distributor', 957), ('variety', 958), ('\"netflix', 959), ('original\"', 960), ('cable', 961), ('channel', 962), ('effort', 963), ('secure', 964), ('right', 965), ('additional', 966), ('diversity', 967), ('result', 968), ('rack', 969), ('debt:', 970), ('previous', 971), ('long-term', 972), ('debt', 973), ('obligation', 974), ('raise', 975), ('another', 976), ('fund', 977), ('whose', 978), ('specialise', 979), ('entertainment', 980), ('globally', 981), ('twin-skyscrapers', 982), ('seafront', 983), ('tower', 984), ('(also', 985), ('binhai', 986), ('mansion)', 987), ('nanshan', 988), ('district', 989), ('shenzhen', 990), ('gaming', 991), ('multiplayer', 992), ('successful', 993), ('respective', 994), ('category', 995), ('qq', 996), ('qqcom', 997), ('(tencent', 998), ('entertainment)', 999), ('cross', 1000), ('emerge', 1001), (\"asia's\", 1002), ('credit', 1003), ('hundred', 1004), ('associate', 1005), ('broad', 1006), ('ownership', 1007), ('across', 1008), ('estate', 1009), ('ride-sharing', 1010), ('banking', 1011), ('fintech', 1012), ('automobile', 1013), ('movie', 1014), ('ticket', 1015), ('space', 1016), ('natural', 1017), ('resource', 1018), ('agriculture', 1019), ('medical', 1020), ('robotics', 1021), ('uavs', 1022), ('courier', 1023), ('e-book', 1024), ('renewable', 1025), ('energy', 1026), ('stakes', 1027), ('recent', 1028), ('start-ups', 1029), ('asiaâ€™s', 1030), ('burgeon', 1031), ('scene', 1032), ('toyota', 1033), ('motor', 1034), ('japanese', 1035), ('automotive', 1036), ('aichi', 1037), (\"toyota's\", 1038), ('structure', 1039), ('consist', 1040), ('sixth-largest', 1041), ('vehicle', 1042), ('per', 1043), ('report', 1044), ('-millionth', 1045), ('(worth', 1046), ('twice', 1047), ('much', 1048), ('-ranked', 1049), ('softbank)', 1050), ('leader', 1051), ('hybrid', 1052), ('electric', 1053), ('encourage', 1054), ('mass-market', 1055), ('adoption', 1056), ('globe', 1057), ('hydrogen', 1058), ('fuel-cell', 1059), ('cumulative', 1060), ('lexus', 1061), ('passenger', 1062), ('car', 1063), ('achieve', 1064), ('milestone', 1065), ('prius', 1066), ('family', 1067), ('selling', 1068), ('nameplate', 1069), ('unit', 1070), ('kiichiro', 1071), ('toyoda', 1072), ('spinoff', 1073), (\"father's\", 1074), ('earlier', 1075), ('still', 1076), ('department', 1077), ('type', 1078), ('aa', 1079), ('hino', 1080), ('ranz', 1081), ('daihatsu', 1082), ('stake', 1083), ('subaru', 1084), ('isuzu', 1085), ('mazda', 1086), ('joint-ventures', 1087), ('(gac', 1088), ('sichuan', 1089), ('faw', 1090), ('motor)', 1091), ('(toyota', 1092), ('kirloskar)', 1093), ('czech', 1094), ('republic', 1095), ('(tpca)', 1096), ('\"nonautomotive\"', 1097), ('tmc', 1098), ('part', 1099), ('london', 1100), ('exchange', 1101), ('tokyo', 1102), ('twitter', 1103), ('interact', 1104), ('message', 1105), ('\"tweets\"', 1106), ('tweet', 1107), ('originally', 1108), ('restrict', 1109), ('character', 1110), ('november', 1111), ('limit', 1112), ('double', 1113), ('korean', 1114), ('retweet', 1115), ('unregistered', 1116), ('read', 1117), ('short', 1118), ('(sms)', 1119), ('mobile-device', 1120), ('(\"app\")', 1121), ('san', 1122), ('francisco', 1123), ('march', 1124), ('jack', 1125), ('dorsey', 1126), ('noah', 1127), ('glass', 1128), ('biz', 1129), ('stone', 1130), ('evan', 1131), ('williams', 1132), ('rapidly', 1133), ('gain', 1134), ('popularity', 1135), ('handle', 1136), ('query', 1137), ('most-visited', 1138), ('describe', 1139), ('\"the', 1140), ('sm', 1141), ('internet\"', 1142), ('hotbed', 1143), ('debate', 1144), ('covering', 1145), ('politics', 1146), ('presidential', 1147), ('election', 1148), ('breaking', 1149), ('election-related', 1150), ('sent', 1151), (':', 1152), ('pm', 1153), ('(eastern', 1154), ('time)', 1155), ('hypermarket', 1156), ('discount', 1157), ('grocery', 1158), ('bentonville', 1159), ('arkansas', 1160), ('sam', 1161), ('walton', 1162), (\"sam's\", 1163), ('club', 1164), ('warehouse', 1165), ('names', 1166), ('de', 1167), ('mÃ©xico', 1168), ('centroamÃ©rica', 1169), ('mexico', 1170), ('central', 1171), ('asda', 1172), ('kingdom', 1173), ('seiyu', 1174), ('wholly', 1175), ('argentina', 1176), ('chile', 1177), ('africa', 1178), ('minority', 1179), ('brasil', 1180), ('private', 1181), ('equity', 1182), ('advent', 1183), ('revenueâ€”over', 1184), ('accord', 1185), ('â€”as', 1186), ('family-owned', 1187), (\"walton's\", 1188), ('heir', 1189), ('individual', 1190), (\"walmart's\", 1191), ('terms', 1192), ('geographically', 1193), ('lower', 1194), ('midwest', 1195), ('early', 1196), ('coast', 1197), ('coast:', 1198), ('open', 1199), ('jersey', 1200), ('outlet', 1201), ('lancaster', 1202), ('pennsylvania', 1203), ('main', 1204), ('northeast', 1205), ('see', 1206), ('results:', 1207), ('highly', 1208), ('whereas', 1209), ('germany', 1210), ('yahoo!', 1211), ('sunnyvale', 1212), ('verizon', 1213), ('jerry', 1214), ('yang', 1215), ('david', 1216), ('filo', 1217), ('yahoo', 1218), ('pioneer', 1219), ('finance', 1220), ('answer', 1221), ('fantasy', 1222), ('sport', 1223), ('height', 1224), ('popular', 1225), ('site', 1226), ('third-party', 1227), ('analytics', 1228), ('alexa', 1229), ('similarweb', 1230), ('widely', 1231), ('sixth-most-visited', 1232), ('slowly', 1233), ('decline', 1234), ('starting', 1235), ('late', 1236), ('communications', 1237), (\"yahoo's\", 1238), ('exclude', 1239), ('transfer', 1240), ('successor', 1241), ('altaba', 1242), ('despite', 1243), ('prominence', 1244), ('domain', 1245), ('(commonly', 1246), ('stylize', 1247), ('intel)', 1248), ('santa', 1249), ('clara', 1250), ('silicon', 1251), ('chip', 1252), ('overtake', 1253), ('inventor', 1254), ('x', 1255), ('microprocessor', 1256), ('processor', 1257), ('(pcs)', 1258), ('supply', 1259), ('hp', 1260), ('dell', 1261), ('motherboard', 1262), ('chipsets', 1263), ('controller', 1264), ('integrate', 1265), ('circuit', 1266), ('flash', 1267), ('graphics', 1268), ('chips', 1269), ('robert', 1270), ('noyce', 1271), ('gordon', 1272), ('moore', 1273), ('(of', 1274), (\"moore's\", 1275), ('law)', 1276), ('vision', 1277), ('grove', 1278), ('conceive', 1279), ('words', 1280), ('co-founder', 1281), ('key', 1282), ('fact', 1283), ('\"intel\"', 1284), ('term', 1285), ('appropriate', 1286), ('developer', 1287), ('sram', 1288), ('dram', 1289), ('represent', 1290), ('commercial', 1291), ('(pc)', 1292), ('invest', 1293), ('heavily', 1294), ('fostering', 1295), ('period', 1296), ('supplier', 1297), ('aggressive', 1298), ('tactics', 1299), ('defense', 1300), ('position', 1301), ('particularly', 1302), ('advance', 1303), ('micro', 1304), ('(amd)', 1305), ('direction', 1306), ('center', 1307), ('powertop', 1308), ('latencytop', 1309), ('open-source', 1310), ('project', 1311), ('wayland', 1312), ('mesad', 1313), ('building', 1314), ('block', 1315), ('thread', 1316), ('(tbb)', 1317), ('xen', 1318), ('sony', 1319), ('kÅnan', 1320), ('minato', 1321), ('industry[better', 1322), ('needed]', 1323), ('parent', 1324), ('engage', 1325), ('components:', 1326), ('(av', 1327), ('&', 1328), ('communication', 1329), ('business)', 1330), ('(movies', 1331), ('shows)', 1332), ('comprehensive', 1333), ('interactive', 1334), ('sony/atv', 1335), ('leaders', 1336), ('fifth-largest', 1337), ('lg', 1338), ('tcl', 1339), ('hisense', 1340), ('current', 1341), ('former', 1342), ('(â€“)', 1343), ('likenoother', 1344), ('makebelieve', 1345), ('weak', 1346), ('tie', 1347), ('sumitomo', 1348), ('mitsui', 1349), ('(smfg)', 1350)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p73OT5sPs9y",
        "colab_type": "code",
        "outputId": "39e8babf-2332-423b-f0d5-343856d826d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "def calc_cosine(vector, vector_list):\n",
        "  result = {}\n",
        "  for i, x in enumerate(vector_list):\n",
        "    result[i] = cosine_similarity(vector, vector_list[i])\n",
        "    \n",
        "  return result\n",
        "\n",
        "print(\"tfidf\")\n",
        "res = calc_cosine(tfidf_vector[0],tfidf_vector)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfidf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0,\n",
              " 1: 0.04945156965230687,\n",
              " 2: 0.03550026859810149,\n",
              " 3: 0.07494324927746153,\n",
              " 4: 0.02200165046387345,\n",
              " 5: 0.089213868005443,\n",
              " 6: 0.04329186935344452,\n",
              " 7: 0.04340970910393382,\n",
              " 8: 0.050616794433693456,\n",
              " 9: 0.05446867547327852,\n",
              " 10: 0.03479541972998953,\n",
              " 11: 0.03392463518350004,\n",
              " 12: 0.038469390607195876,\n",
              " 13: 0.05035814117836253,\n",
              " 14: 0.06794378321355649,\n",
              " 15: 0.029516361108928312}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMGEF5UJUZSz",
        "colab_type": "code",
        "outputId": "f311182a-318b-4dae-bb89-b58693a0d2ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "sorted(res.items(), key=lambda x:x[1],reverse=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1.0),\n",
              " (5, 0.089213868005443),\n",
              " (3, 0.07494324927746153),\n",
              " (14, 0.06794378321355649),\n",
              " (9, 0.05446867547327852),\n",
              " (8, 0.050616794433693456),\n",
              " (13, 0.05035814117836253),\n",
              " (1, 0.04945156965230687),\n",
              " (7, 0.04340970910393382),\n",
              " (6, 0.04329186935344452),\n",
              " (12, 0.038469390607195876),\n",
              " (2, 0.03550026859810149),\n",
              " (10, 0.03479541972998953),\n",
              " (11, 0.03392463518350004),\n",
              " (15, 0.029516361108928312),\n",
              " (4, 0.02200165046387345)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OWm8pnAMQH-",
        "colab_type": "text"
      },
      "source": [
        "## Option 3\n",
        "\n",
        "### Word2Vec & Doc2Vec\n",
        "\n",
        "Word2Vecã‚„Doc2Vecã§ã¯å˜èªã®æ„å‘³ã‚’æ‰ãˆã‚‰ã‚Œã¦ã„ã‚‹ã‹ã®ã‚ˆã†ãªæ¼”ç®—ãŒå‡ºæ¥ã‚‹  \n",
        "King - Man + Woman = Queen ãªã©  \n",
        "è©³ç´°ã¯è¬›ç¾©ã‚¹ãƒ©ã‚¤ãƒ‰ã¸   \n",
        "\n",
        "å­¦ç¿’æ¸ˆã¿ã®word2vecãŒgithub( https://github.com/Kyubyong/wordvectors )ã«ä¸ŠãŒã£ã¦ã„ã‚‹ã®ã§  \n",
        "æ—¥æœ¬ã¨å„å›½ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã¦ã¿ã‚ˆã†  \n",
        "è¶³ã—ç®—ã‚„å¼•ãç®—ãŒå‡ºæ¥ã‚‹ã®ã§ãã‚Œã‚‚è©¦ã—ã¦ã¿ã‚ˆã†  \n",
        "\n",
        "å‚è€ƒ : \"BOKU\"ã®ITãªæ—¥å¸¸ (https://arakan-pgm-ai.hatenablog.com/entry/2019/02/08/090000)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWddZ1mZXFsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
