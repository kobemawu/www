{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similarity_EN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kobemawu/www/blob/master/Similarity_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5GKSZugz6zl"
      },
      "source": [
        "# Data preprocessing and similarity calculation\n",
        "This notebook will help you to calculate the similarity of documents by yourself.  \n",
        "Finally, you can calculate the similarity of different countries with Wikipedia data and find out countries similar to Japan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZCjxsCaztqM",
        "outputId": "91d26283-9c54-449a-8f24-cb667a9263a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install necessary dependencies\n",
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vjf9tGp1LoV"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G954jKDz1QJR",
        "outputId": "998c1c31-c1d3-4989-d010-d20012edb177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oAqjfZQ1Khd"
      },
      "source": [
        "## 1. Similarity caluculation\n",
        "First, let us consider the following three documents:  \n",
        "Doc A : \"I like apples and strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"   \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"   \n",
        "\n",
        "\n",
        "Doc A seems similar to Doc B while Doc C seems different from Doc A or Doc B.  \n",
        "Let us confirm our guess through similarity calculation.\n",
        "\n",
        "There are several methods to calculate the similarity:  \n",
        "* Set-based similarity\n",
        " * Jaccard index\n",
        " * S√∏rensen‚ÄìDice coefficient\n",
        " * Overlap coefficient  (Szymkiewicz‚ÄìSimpson coefficient)\n",
        "* Vector-based similarity\n",
        " * Euclidean distance\n",
        " * Consine similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe66YblJAgS7"
      },
      "source": [
        "### Set based similarity\n",
        "Convert the words of documents to sets.   \n",
        "Dupliate words will be eliminated due to the setting of sets.   \n",
        "Here is an example and the preprocess is skipped.   \n",
        "\n",
        "Doc A : \"I like apples and strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "‚Üì    \n",
        "Set A : {'a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'}  \n",
        "Set B : {'an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'}  \n",
        "Set C : {'basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'}  \n",
        "\n",
        "Let us consider how to represent the features of the above word sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5gN1DTSCqex"
      },
      "source": [
        "### Jaccard index\n",
        "Jaccard index defines the similarity and diversity of two sample sets A and B:   \n",
        "\n",
        "\\begin{equation}\n",
        "J(A,B)=\\dfrac{|A\\cap B|}{|A \\cup B|}\n",
        "\\end{equation}\n",
        "\n",
        "The greater the proportion of intersections, the more similar the two documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKLgZy_IAGtK"
      },
      "source": [
        "def jaccard_similarity(set_a,set_b):\n",
        "  # calculate the intersection\n",
        "  num_intersection = len(set.intersection(set_a, set_b))\n",
        "  # calculate the union\n",
        "  num_union = len(set.union(set_a, set_b))\n",
        "  # calculate the Jaccard index, return 1 if the set is empty\n",
        "  try:\n",
        "      return float(num_intersection) / num_union\n",
        "  except ZeroDivisionError:\n",
        "      return 1.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTCa6kRuSDmG",
        "outputId": "97443aff-1b39-4245-ab06-cf1b75f11622",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"jaccard(a, b) = \", jaccard_similarity(set_a, set_b)) # Get the Jaccard index of the two sets\n",
        "print(\"jaccard(a, c) = \", jaccard_similarity(set_a, set_c))\n",
        "print(\"jaccard(b, c) = \", jaccard_similarity(set_b, set_c))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard(a, b) =  0.5714285714285714\n",
            "jaccard(a, c) =  0.11764705882352941\n",
            "jaccard(b, c) =  0.05555555555555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tQ4c3DSSQab"
      },
      "source": [
        "Jaccard index is also implemented by nltk package.  \n",
        "The Input is sets and it will calculate according to the definition.  \n",
        "Please note that the output is the distance not the similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUAGENO-T1ob",
        "outputId": "b19208c6-1c85-4268-8da3-972a6f09b07d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.metrics import jaccard_distance\n",
        "\n",
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "# Note the output of the nltk package is distance, using 1 to minus the result to convert to similarity. \n",
        "print(\"jaccard(a, b) = \", 1 - jaccard_distance(set_a, set_b))\n",
        "print(\"jaccard(a, c) = \", 1 - jaccard_distance(set_a, set_c))\n",
        "print(\"jaccard(b, c) = \", 1 - jaccard_distance(set_b, set_c))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard(a, b) =  0.5714285714285714\n",
            "jaccard(a, c) =  0.11764705882352944\n",
            "jaccard(b, c) =  0.05555555555555558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J_pwtgGVZBR"
      },
      "source": [
        "### S√∏rensen‚ÄìDice coefficient\n",
        "The problem of Jaccard index is that if the size of one set is too large, the value is small no matter how large of the intersection.  \n",
        "In order to improve the problem, S√∏rensen‚ÄìDice coefficient sets the denominator as the average value of the two sets.  \n",
        "\n",
        "$\n",
        "DSC(A,B) = \\dfrac{|A\\cap B|}{\\dfrac{|A| + |B|}{2}} = \\dfrac{2|A\\cap B|}{|A| + |B|}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2vBT38bW_UQ"
      },
      "source": [
        "def dice_similarity(set_a, set_b):\n",
        "  num_intersection =  len(set.intersection(set_a, set_b))\n",
        "  sum_nums = len(set_a) + len(set_b)\n",
        "  try:\n",
        "    return 2 * num_intersection / sum_nums\n",
        "  except ZeroDivisionError:\n",
        "    return 1.0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a24_x9MXVEW",
        "outputId": "b92f1c8c-3eeb-4556-fc6c-d38cb0b601f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"dice(a, b) = \", dice_similarity(set_a, set_b))\n",
        "print(\"dice(a, c) = \", dice_similarity(set_a, set_c))\n",
        "print(\"dice(b, c) = \", dice_similarity(set_b, set_c))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dice(a, b) =  0.7272727272727273\n",
            "dice(a, c) =  0.21052631578947367\n",
            "dice(b, c) =  0.10526315789473684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdqY-mcTXXnJ"
      },
      "source": [
        "### Overlap coefficient\n",
        "Overlap coefficient is defined as the size of the intersection divided by the smaller of the size of the two sets.\n",
        "\n",
        "$\n",
        "overlap(ùê¥,ùêµ) = \\dfrac{|A\\cap B|}{\\min(|A|, |B|)}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ6-eK4wesF_"
      },
      "source": [
        "def simpson_similarity(set_a, set_b):\n",
        "  num_intersection = len(set.intersection(set_a, set_b))\n",
        "  min_num = min(len(set_a), len(set_b))\n",
        "  try:\n",
        "    return num_intersection / min_num\n",
        "  except ZeroDivisionError:\n",
        "    if num_intersection == 0:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ0WuOGOfIMh",
        "outputId": "1b975ad2-c7bd-4e46-f898-18312040a5e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "\n",
        "print(\"simpson(a, b) = \", simpson_similarity(set_a, set_b)) \n",
        "print(\"simpson(a, c) = \", simpson_similarity(set_a, set_c)) \n",
        "print(\"simpson(b, c) = \", simpson_similarity(set_b, set_c)) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simpson(a, b) =  0.7272727272727273\n",
            "simpson(a, c) =  0.25\n",
            "simpson(b, c) =  0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhyu1yGCfMzi"
      },
      "source": [
        "#### Exercise 1\n",
        "Create different kinds of sets and compare the result with set-based similarity calculation methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3HVinaJfLtO",
        "outputId": "89c98005-8aeb-4b57-ed80-09735fa8ed0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set_a = set(['a', 'an', 'and', 'apple', 'apples', 'buy', 'i', 'like', 'strawberries', 'tomorrow', 'will'])\n",
        "set_b = set(['an', 'and', 'apple', 'apples', 'bought', 'eat', 'i', 'some', 'strawberries', 'tomorrow', 'will'])\n",
        "set_c = set(['basketball', 'day', 'every', 'i', 'jordan', 'like', 'michael', 'play'])\n",
        "set_d = set() # Try it with large size of sets.\n",
        "\n",
        "print(\"jaccard similarity:\")\n",
        "print(jaccard_similarity(set_d, set_a))\n",
        "print(jaccard_similarity(set_d, set_b))\n",
        "print(jaccard_similarity(set_d, set_c))\n",
        "\n",
        "print(\"dice similarity:\")\n",
        "print(dice_similarity(set_d, set_a))\n",
        "print(dice_similarity(set_d, set_b))\n",
        "print(dice_similarity(set_d, set_c))\n",
        "\n",
        "print(\"simpson similarity:\")\n",
        "print(simpson_similarity(set_d, set_a))\n",
        "print(simpson_similarity(set_d, set_b))\n",
        "print(simpson_similarity(set_d, set_c))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard similarity:\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "dice similarity:\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "simpson similarity:\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ6fX4vqiL7K"
      },
      "source": [
        "### Vector based similarity\n",
        "Calculate similarity on the vectorized documents.  \n",
        "There are many vectorization methods and we use BoW (Bag of Words) in this tutorial.  \n",
        "\n",
        "BoW is one of the document vectorization methods.  \n",
        "Suppose the total number of words is N, each document is represented as an N-dimension vector. Each dimension represents one word and the value of each dimension is the number of occurrences of the word in the document.  \n",
        "\n",
        "For example:\n",
        "Doc A : \"I like apples and strawberries. I will buy an apple tomorrow. \"  \n",
        "Doc B : \"I bought some apples and strawberries. I will eat an apple tomorrow.\"  \n",
        "Doc C : \"I play basketball every day. I like Michael Jordan.\"  \n",
        "‚Üì  \n",
        "The total number of words is 19. Therefore, the value of each dimension in BoW is decided by the number of occurrences of the following words.   \n",
        "['an', 'and', 'apple', 'apples', 'basketball', 'bought', 'buy', 'day', 'eat', 'every', 'i', 'jordan', 'like', 'michael', 'play', 'some', 'strawberries', 'tomorrow', 'will']  \n",
        "‚Üì  \n",
        "BoW A : [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]  \n",
        "BoW B : [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
        "BoW C : [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
        "\n",
        "These vectors represent features of the documents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_4j6MrIsvCi"
      },
      "source": [
        "#### Euclidean distance\n",
        "We can calculate the Euclidean distance on the vectorized documents.  \n",
        "The closer the two documents are, the more similar they are.   \n",
        "\n",
        "\\begin{equation}\n",
        "d(v_1,v_2) =(\\sum_{i=1}^n (v_{1i}-v_{2i})^2)^{\\frac{1}{2}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAHCOKaosMmX"
      },
      "source": [
        "def euclidean_distance(list_a, list_b):\n",
        "  diff_vec = np.array(list_a) - np.array(list_b)\n",
        "  return np.linalg.norm(diff_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_4dlRWAt4Q1",
        "outputId": "20799644-de61-4478-9a16-5dd3c1a31d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bow_a = [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]  \n",
        "bow_b = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]  \n",
        "bow_c = [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]  \n",
        "\n",
        "print(\"euclidean_distance(bow_a, bow_b) = \",euclidean_distance(bow_a, bow_b))\n",
        "print(\"euclidean_distance(bow_a, bow_c) = \",euclidean_distance(bow_a, bow_c))\n",
        "print(\"euclidean_distance(bow_b, bow_c) = \",euclidean_distance(bow_b, bow_c))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "euclidean_distance(bow_a, bow_b) =  2.23606797749979\n",
            "euclidean_distance(bow_a, bow_c) =  3.7416573867739413\n",
            "euclidean_distance(bow_b, bow_c) =  4.123105625617661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL1LwBTBt6hK"
      },
      "source": [
        "#### Minkowski distance\n",
        "A generalization of Euclidean distance.  \n",
        "It can represent different kinds of distance by changing the value of ```p```\n",
        "\n",
        "\\begin{equation}\n",
        "d(v_1,v_2) = (\\sum_{i=1}^n |v_{1i}-v_{2i}|^p)^{\\frac{1}{p}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqnxAEHyvnPN"
      },
      "source": [
        "#### Exercise 2\n",
        "Try to implement a function to compute the Minkowski distance.  \n",
        "Try it with different ```p``` value (e,g., p = 1, 2, 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdtSvWN3vj9T"
      },
      "source": [
        "# please refer to np.linalg.norm\n",
        "def minkowski_distance(list_a, list_b, p):\n",
        "  #please complete the code. \n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBT2MxVowCMc"
      },
      "source": [
        "# p=1\n",
        "print(minkowski_distance(bow_a, bow_b, 1))\n",
        "print(minkowski_distance(bow_a, bow_c, 1))\n",
        "print(minkowski_distance(bow_b, bow_c, 1))\n",
        "\n",
        "# p=2\n",
        "print(minkowski_distance(bow_a, bow_b, 2))\n",
        "print(minkowski_distance(bow_a, bow_c, 2))\n",
        "print(minkowski_distance(bow_b, bow_c, 2))\n",
        "\n",
        "# p=3\n",
        "print(minkowski_distance(bow_a, bow_b, 3))\n",
        "print(minkowski_distance(bow_a, bow_c, 3))\n",
        "print(minkowski_distance(bow_b, bow_c, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWz6SGyjwBbp"
      },
      "source": [
        "#### Cosine similarity\n",
        "Measure the similarity of two vectors by computing the cosine of the angle between them.  \n",
        "\n",
        "\\begin{equation}\n",
        "similarity(A, B)=cos(\\theta)=\\dfrac{\\sum_{i=1}^n A_iB_i}{{\\sqrt A}{\\sqrt B}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw-9iSn5ygn5"
      },
      "source": [
        "#### Exercise 3\n",
        "Try to implement a function to compute cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18-kBkozydv-"
      },
      "source": [
        "# please refer to numpy.array and np.linalg.norm\n",
        "def cosine_similarity(list_a, list_b):\n",
        "  #please complete the code.\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUZudWcGz0IN",
        "outputId": "b6145e4f-b933-4e3a-b81d-57c13f3bdb92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "bow_a = [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1]\n",
        "bow_b = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1]\n",
        "bow_c = [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "print(\"cosine_similarity(bow_a, bow_b) = \",cosine_similarity(bow_a, bow_b))\n",
        "print(\"cosine_similarity(bow_a, bow_c) = \",cosine_similarity(bow_a, bow_c))\n",
        "print(\"cosine_similarity(bow_b, bow_c) = \",cosine_similarity(bow_b, bow_c))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine_similarity(bow_a, bow_b) =  0.8153742483272114\n",
            "cosine_similarity(bow_a, bow_c) =  0.41812100500354543\n",
            "cosine_similarity(bow_b, bow_c) =  0.3223291856101521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWkipAxH0DoO"
      },
      "source": [
        "### Compare set-based and vector-based similarity calculation methods\n",
        "Set based similarity calculation is more efficient because the computation is done on the small piece of word sets.  \n",
        "If the size of the document is large enough, vector based methods are more useful.  \n",
        "However, the calculation amount will increase as the corpus increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTesl0Ui7x8C"
      },
      "source": [
        "### Exercise 4\n",
        "Create some short and long document datasets, try to compare the results of Jaccard index with cosine similarity. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dXabVNh8TL8"
      },
      "source": [
        "short_docs = []\n",
        "long_docs = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-9wNbu8UDv"
      },
      "source": [
        "## 2. Preprocessing\n",
        "Now we can calculate the similarity of documents by computing the union of sets or angle of vectors.  \n",
        "However, if the sets and vectors cannot represent the features of the documents properly, the calculated similarity might not accurate enough.  \n",
        "Therefore, data proprecessing such as convert documetns to sets or vectors is very important. \n",
        "\n",
        "Let us consider how to properly preprocess the documents for similarity calculation.  \n",
        "Then we will concentrate on the training of document vectorization.  \n",
        "1. Clearning\n",
        "2. Tokenize\n",
        "3. Stemming\n",
        "4. Remove stop words\n",
        "5. Vectorize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdeBnySCNG2I"
      },
      "source": [
        "### 2-1. Clearning\n",
        "Although the documents we used in the previous similarity computation looks clean, they were very messy even include HTML tags and strange symbols."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78OjABzwM9cl"
      },
      "source": [
        "documents=[\"I like apples and strawberries. I will buy an apple tomorrow @Fresco.\",\n",
        "           \"I bought some apples and strawberries. I will eat an apple <b>tomorrow.</b>\",\n",
        "           \"I play basketball every day. I like Michael Jordan (born February 17, 1963).\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTQimVo3YxvY"
      },
      "source": [
        "We can delete these strange characters manually because there are only thre documents.  \n",
        "However, when there is a large amount of data than needs to be cleaned up, a cleaning program is necessary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgbcVL-PclVu"
      },
      "source": [
        "#### Exercise 5\n",
        "Try to implement a function with regular expression to clean up text data. \n",
        "\n",
        "Reference: <https://www.w3schools.com/python/python_regex.asp>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfUEKaRYcj7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a414e5-26f5-4308-f442-659a046a6359"
      },
      "source": [
        "import re\n",
        "\n",
        "def cleaning_text(text):\n",
        "    # delete @\n",
        "    pattern1 = '@'\n",
        "    text = re.sub(pattern1, '', text)    \n",
        "    # delete <b> tag; please complete the code\n",
        "    pattern2 = #\n",
        "    text = re.sub(pattern2, '', text)    \n",
        "    # delete the content in the (); please complete the code\n",
        "    pattern3 =  # \n",
        "    text = re.sub(pattern3, '', text)\n",
        "    return text\n",
        "  \n",
        "\n",
        "for text in documents:\n",
        "    print(cleaning_text(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like apples and strawberries. I will buy an apple tomorrow Fresco.\n",
            "I bought some apples and strawberries. I will eat an apple tomorrow.\n",
            "I play basketball every day. I like Michael Jordan .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocvWPFIVeuWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6c8214-c790-43da-f989-222c9382b5cc"
      },
      "source": [
        "for text in documents:\n",
        "  print(cleaning_text(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like apples and strawberries. I will buy an apple tomorrow Fresco.\n",
            "I bought some apples and strawberries. I will eat an apple tomorrow.\n",
            "I play basketball every day. I like Michael Jordan .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrwJVvsDeyJq"
      },
      "source": [
        "#### Option 1\n",
        "\n",
        "Try to clean up the following text with your implemented cleaning function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWopWp6Hfcru"
      },
      "source": [
        "text = '<p><b>Natural language processing</b> (<b>NLP</b>) is a subfield of <a href=\"/wiki/Computer_science\" title=\"Computer science\">computer science</a>, <a href=\"/wiki/Information_engineering_(field)\" title=\"Information engineering (field)\">information engineering</a>, and <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of <a href=\"/wiki/Natural_language\" title=\"Natural language\">natural language</a> data.</p>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-7dJ5iwff__"
      },
      "source": [
        "### 2-2. Tokenize\n",
        "After cleaning, you need to cut each word from document strings.  \n",
        "English words can be separated by blank spaces, while Japanese words are more complicated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvE6D0dHhPNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010b1822-5573-46c6-dfbd-892e8eee3bca"
      },
      "source": [
        "def tokenize_text(text):\n",
        "  text = re.sub('[.,]', '', text)\n",
        "  return text.split()\n",
        "\n",
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  print(tokenize_text(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'like', 'apples', 'and', 'strawberries', 'I', 'will', 'buy', 'an', 'apple', 'tomorrow', 'Fresco']\n",
            "['I', 'bought', 'some', 'apples', 'and', 'strawberries', 'I', 'will', 'eat', 'an', 'apple', 'tomorrow']\n",
            "['I', 'play', 'basketball', 'every', 'day', 'I', 'like', 'Michael', 'Jordan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJlxsXNehSOD"
      },
      "source": [
        "### 2-3. Stemming, Lemmatize\n",
        "The same word may have multiple forms, and it would be a bit strange if we treat them as different words.  \n",
        "So after converting to lowercase, we use **Stemming** and **Lemmatize** to turn words into the uniform format.  \n",
        "Here we only show the lemmatize. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTX5a29RjspQ"
      },
      "source": [
        "from nltk.corpus import wordnet as wn # import lemmatize\n",
        "\n",
        "def lemmatize_word(word):\n",
        "    # make words lower  e.g., Python =>python\n",
        "    word=word.lower()\n",
        "    \n",
        "    # lemmatize  e.g., cooked=>cook\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "      return lemma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubSJUz5tjwfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2be0a1-ea0f-42f2-a0ca-744d2ed33daf"
      },
      "source": [
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  print([lemmatize_word(word) for word in tokens])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'like', 'apple', 'and', 'strawberry', 'i', 'will', 'buy', 'an', 'apple', 'tomorrow', 'fresco']\n",
            "['i', 'buy', 'some', 'apple', 'and', 'strawberry', 'i', 'will', 'eat', 'an', 'apple', 'tomorrow']\n",
            "['i', 'play', 'basketball', 'every', 'day', 'i', 'like', 'michael', 'jordan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z5C5mauj603"
      },
      "source": [
        "We can find out that 'strawberries' is converted to the standard form 'strawberry'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBQlMVaBkYN5"
      },
      "source": [
        "### 2-4. Remove stop words\n",
        "There are many words such as 'a' and 'the', which have no effects on the meaning of the documents.   \n",
        "These words are called stop words.  \n",
        "We can apply the stopwords list from nltk package which is defined by specialists.  \n",
        "You can define your own stopwords if necessary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jha5WyDilrYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4400871a-8e15-418e-968d-bd1c48369add"
      },
      "source": [
        "en_stop = nltk.corpus.stopwords.words('english')\n",
        "print(en_stop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unaB2VPUmSJA"
      },
      "source": [
        "def remove_stopwords(word, stopwordset):\n",
        "  if word in stopwordset:\n",
        "    return None\n",
        "  else:\n",
        "    return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylr7X0dTmSy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff131efd-97c3-443b-f2cd-12074fa271a3"
      },
      "source": [
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  print([remove_stopwords(word, en_stop) for word in tokens])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 'like', 'apple', None, 'strawberry', None, None, 'buy', None, 'apple', 'tomorrow', 'fresco']\n",
            "[None, 'buy', None, 'apple', None, 'strawberry', None, None, 'eat', None, 'apple', 'tomorrow']\n",
            "[None, 'play', 'basketball', 'every', 'day', None, 'like', 'michael', 'jordan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z6sZhhKlonI"
      },
      "source": [
        "We only show a simple preprocessing of stopwords in this tutorial.  \n",
        "In the real application scenario, you may need to delete low occurrence frequency words or determiners of verbs and nouns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W6uUXHsplR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec1c887-ecde-443b-93f3-937bcdf240f1"
      },
      "source": [
        "def preprocessing_text(text):\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
        "  tokens = [word for word in tokens if word is not None]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "preprocessed_docs = [preprocessing_text(text) for text in documents]\n",
        "preprocessed_docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['like', 'apple', 'strawberry', 'buy', 'apple', 'tomorrow', 'fresco'],\n",
              " ['buy', 'apple', 'strawberry', 'eat', 'apple', 'tomorrow'],\n",
              " ['play', 'basketball', 'every', 'day', 'like', 'michael', 'jordan']]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnP5Lgnpps-B"
      },
      "source": [
        "### 2-5. Vectorize\n",
        "#### BoW(Bag of Words)\n",
        "Convert text to a vector with the occurrence time of words.  \n",
        "It is impossible to do this manually, so we need to write a function to handle it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvO58vMBpoj5"
      },
      "source": [
        "def bow_vectorizer(docs):\n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "        \n",
        "  result_list = []\n",
        "  for doc in docs:\n",
        "    doc_vec = [0] * len(word2id)\n",
        "    for w in doc:\n",
        "      doc_vec[word2id[w]] += 1\n",
        "    result_list.append(doc_vec)\n",
        "  return result_list, word2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlLcr5eHqvUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391f23e3-c663-4a69-a1ac-c6cb53385aec"
      },
      "source": [
        "bow_vec, word2id = bow_vectorizer(preprocessed_docs)\n",
        "print(bow_vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w970Q00Hqv49"
      },
      "source": [
        "word2id.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ePbI8Lwq1cA"
      },
      "source": [
        "### TF-IDF(Term Frequency - Inverse Document Frequency)\n",
        "BoW treats every word with the same weight, but different words may have different degrees of importance.  \n",
        "\n",
        "TF-IDF improve this problem by considering the word's weight.  \n",
        "TF(t, d) = the frequency of the word (t) in the document.   \n",
        "IDF(t) = the inverse frequency of the word (t) in all documents.  \n",
        "\n",
        "TF-IDF(t,d) = TF(t, d) * IDF(t)  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxb-QxaAsgEz"
      },
      "source": [
        "def tfidf_vectorizer(docs):\n",
        "  def tf(word2id, doc):\n",
        "    term_counts = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      term_counts[word2id[term]] = doc.count(term)\n",
        "    tf_values = list(map(lambda x: x/sum(term_counts), term_counts))\n",
        "    return tf_values\n",
        "  \n",
        "  def idf(word2id, docs):\n",
        "    idf = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      idf[word2id[term]] = np.log(len(docs) / sum([bool(term in doc) for doc in docs]))\n",
        "    return idf\n",
        "  \n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "  \n",
        "  return [[_tf*_idf for _tf, _idf in zip(tf(word2id, doc), idf(word2id, docs))] for doc in docs], word2id\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgdPPHBUsT0z"
      },
      "source": [
        "tfidf_vector, word2id = tfidf_vectorizer(preprocessed_docs)\n",
        "print(tfidf_vector)\n",
        "print(word2id.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnB0xCtNskJb"
      },
      "source": [
        "#### Exercise 6\n",
        "Calculate the cosine similarity of documents with BoW and TF-IDF.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgukCHVUs7kt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqGZFa7as_0U"
      },
      "source": [
        "#### Option 2\n",
        "TF-IDF is implemented in different packages and each has different parameters.  \n",
        "Calculate TF-IDF with different packages (e.g., scikit-learn, nltk, gensim)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97fI1Mc9ta8C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P951KgUFthAq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2JafXEmthfp"
      },
      "source": [
        "## Exercise 7\n",
        "Here is a dataset that includes different countries' abstract from Wikipedia.  \n",
        "Please download it through the URL:  \n",
        "https://drive.google.com/open?id=1i7tekPQRKaAwg-ze3kv5IsufMW13LkLo   \n",
        "\n",
        "Calculate the similarity of different countries and find out the top-5 countries similar to Japan. \n",
        "Please note that the data preprocessing should done by yourself.   \n",
        "P.S. It is fine if the similarity value is not very high. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uootd6SEvhc-"
      },
      "source": [
        "## Option 3\n",
        "\n",
        "### Word2Vec & Doc2Vec\n",
        "Word2Vec and Doc2Vec are very popular tools which can handle with the meaning of words.  \n",
        "For example: King - Man + Woman = Queen   \n",
        "Detail can be found in the slides.\n",
        "\n",
        "The Pre-trained model of word2vec can be found from the following URL:  \n",
        "https://github.com/Kyubyong/wordvectors \n",
        "\n",
        "Try to calculate the similarity between Japan and other countries.  \n",
        "Try addition or subtraction between the two countries.  \n",
        "Reference: http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAsnQO2vguW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}